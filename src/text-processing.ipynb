{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79ace40a",
   "metadata": {
    "papermill": {
     "duration": 0.004784,
     "end_time": "2025-09-12T07:38:08.017036",
     "exception": false,
     "start_time": "2025-09-12T07:38:08.012252",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# NLP - Pre-processing\n",
    "\n",
    "Corpus -> Paragraph\n",
    "\n",
    "Documents -> Sentences\n",
    "\n",
    "Vocabulary -> Unique Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00cba99",
   "metadata": {
    "papermill": {
     "duration": 0.003996,
     "end_time": "2025-09-12T07:38:08.025426",
     "exception": false,
     "start_time": "2025-09-12T07:38:08.021430",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Tokenization\n",
    "\n",
    "It is the fundamental process of breaking down a text into smaller, manageable units called `tokens`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed656b12",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-12T07:38:08.034332Z",
     "iopub.status.busy": "2025-09-12T07:38:08.033996Z",
     "iopub.status.idle": "2025-09-12T07:38:10.142520Z",
     "shell.execute_reply": "2025-09-12T07:38:10.141587Z"
    },
    "papermill": {
     "duration": 2.114856,
     "end_time": "2025-09-12T07:38:10.144192",
     "exception": false,
     "start_time": "2025-09-12T07:38:08.029336",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "from nltk.tokenize import (\n",
    "    sent_tokenize,\n",
    "    word_tokenize,\n",
    "    wordpunct_tokenize,\n",
    "    TreebankWordTokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d5effd1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-12T07:38:10.153956Z",
     "iopub.status.busy": "2025-09-12T07:38:10.153083Z",
     "iopub.status.idle": "2025-09-12T07:38:10.157619Z",
     "shell.execute_reply": "2025-09-12T07:38:10.156748Z"
    },
    "papermill": {
     "duration": 0.010921,
     "end_time": "2025-09-12T07:38:10.159093",
     "exception": false,
     "start_time": "2025-09-12T07:38:10.148172",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "text = \"\"\"There are multiple ways we can perform cost's tokenization on given text data.\n",
    "We can choose any method based on language, library and purpose of modeling.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9816493b",
   "metadata": {
    "papermill": {
     "duration": 0.003611,
     "end_time": "2025-09-12T07:38:10.166755",
     "exception": false,
     "start_time": "2025-09-12T07:38:10.163144",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48425d44",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-12T07:38:10.176044Z",
     "iopub.status.busy": "2025-09-12T07:38:10.175301Z",
     "iopub.status.idle": "2025-09-12T07:38:10.212902Z",
     "shell.execute_reply": "2025-09-12T07:38:10.212070Z"
    },
    "papermill": {
     "duration": 0.043698,
     "end_time": "2025-09-12T07:38:10.214304",
     "exception": false,
     "start_time": "2025-09-12T07:38:10.170606",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"There are multiple ways we can perform cost's tokenization on given text data.\",\n",
       " 'We can choose any method based on language, library and purpose of modeling.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = sent_tokenize(text=text, language='english')\n",
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb4eed7",
   "metadata": {
    "papermill": {
     "duration": 0.004729,
     "end_time": "2025-09-12T07:38:10.223491",
     "exception": false,
     "start_time": "2025-09-12T07:38:10.218762",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0bd9c2c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-12T07:38:10.232830Z",
     "iopub.status.busy": "2025-09-12T07:38:10.232027Z",
     "iopub.status.idle": "2025-09-12T07:38:10.238634Z",
     "shell.execute_reply": "2025-09-12T07:38:10.237778Z"
    },
    "papermill": {
     "duration": 0.012631,
     "end_time": "2025-09-12T07:38:10.239944",
     "exception": false,
     "start_time": "2025-09-12T07:38:10.227313",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['There',\n",
       " 'are',\n",
       " 'multiple',\n",
       " 'ways',\n",
       " 'we',\n",
       " 'can',\n",
       " 'perform',\n",
       " 'cost',\n",
       " \"'s\",\n",
       " 'tokenization',\n",
       " 'on',\n",
       " 'given',\n",
       " 'text',\n",
       " 'data',\n",
       " '.',\n",
       " 'We',\n",
       " 'can',\n",
       " 'choose',\n",
       " 'any',\n",
       " 'method',\n",
       " 'based',\n",
       " 'on',\n",
       " 'language',\n",
       " ',',\n",
       " 'library',\n",
       " 'and',\n",
       " 'purpose',\n",
       " 'of',\n",
       " 'modeling',\n",
       " '.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = word_tokenize(text=text, language='english')\n",
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5a0a8e",
   "metadata": {
    "papermill": {
     "duration": 0.003713,
     "end_time": "2025-09-12T07:38:10.247916",
     "exception": false,
     "start_time": "2025-09-12T07:38:10.244203",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### wordpunct_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "657380f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-12T07:38:10.257093Z",
     "iopub.status.busy": "2025-09-12T07:38:10.256550Z",
     "iopub.status.idle": "2025-09-12T07:38:10.435639Z",
     "shell.execute_reply": "2025-09-12T07:38:10.434744Z"
    },
    "papermill": {
     "duration": 0.185274,
     "end_time": "2025-09-12T07:38:10.437051",
     "exception": false,
     "start_time": "2025-09-12T07:38:10.251777",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['There',\n",
       " 'are',\n",
       " 'multiple',\n",
       " 'ways',\n",
       " 'we',\n",
       " 'can',\n",
       " 'perform',\n",
       " 'cost',\n",
       " \"'\",\n",
       " 's',\n",
       " 'tokenization',\n",
       " 'on',\n",
       " 'given',\n",
       " 'text',\n",
       " 'data',\n",
       " '.',\n",
       " 'We',\n",
       " 'can',\n",
       " 'choose',\n",
       " 'any',\n",
       " 'method',\n",
       " 'based',\n",
       " 'on',\n",
       " 'language',\n",
       " ',',\n",
       " 'library',\n",
       " 'and',\n",
       " 'purpose',\n",
       " 'of',\n",
       " 'modeling',\n",
       " '.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "\n",
    "wordpunct = wordpunct_tokenize(text=text)\n",
    "wordpunct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433be589",
   "metadata": {
    "papermill": {
     "duration": 0.003954,
     "end_time": "2025-09-12T07:38:10.445249",
     "exception": false,
     "start_time": "2025-09-12T07:38:10.441295",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13d73272",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-12T07:38:10.454537Z",
     "iopub.status.busy": "2025-09-12T07:38:10.454225Z",
     "iopub.status.idle": "2025-09-12T07:38:10.459890Z",
     "shell.execute_reply": "2025-09-12T07:38:10.459150Z"
    },
    "papermill": {
     "duration": 0.012053,
     "end_time": "2025-09-12T07:38:10.461302",
     "exception": false,
     "start_time": "2025-09-12T07:38:10.449249",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['There',\n",
       " 'are',\n",
       " 'multiple',\n",
       " 'ways',\n",
       " 'we',\n",
       " 'can',\n",
       " 'perform',\n",
       " 'cost',\n",
       " \"'s\",\n",
       " 'tokenization',\n",
       " 'on',\n",
       " 'given',\n",
       " 'text',\n",
       " 'data.',\n",
       " 'We',\n",
       " 'can',\n",
       " 'choose',\n",
       " 'any',\n",
       " 'method',\n",
       " 'based',\n",
       " 'on',\n",
       " 'language',\n",
       " ',',\n",
       " 'library',\n",
       " 'and',\n",
       " 'purpose',\n",
       " 'of',\n",
       " 'modeling',\n",
       " '.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "treebank = TreebankWordTokenizer().tokenize(text)\n",
    "treebank"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01196d8c",
   "metadata": {
    "papermill": {
     "duration": 0.00402,
     "end_time": "2025-09-12T07:38:10.469602",
     "exception": false,
     "start_time": "2025-09-12T07:38:10.465582",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Stemming\n",
    "\n",
    "It is the process of reducing a word to its base form, known as the `stem`. This `stem` may **not** be a valid word in itself, but it serves as the foundation to which prefixes and suffixes are attached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "571d90bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-12T07:38:10.478939Z",
     "iopub.status.busy": "2025-09-12T07:38:10.478632Z",
     "iopub.status.idle": "2025-09-12T07:38:10.482447Z",
     "shell.execute_reply": "2025-09-12T07:38:10.481790Z"
    },
    "papermill": {
     "duration": 0.010687,
     "end_time": "2025-09-12T07:38:10.484257",
     "exception": false,
     "start_time": "2025-09-12T07:38:10.473570",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.stem import (\n",
    "    PorterStemmer,\n",
    "    SnowballStemmer,\n",
    "    RegexpStemmer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78ec5bde",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-12T07:38:10.494645Z",
     "iopub.status.busy": "2025-09-12T07:38:10.494073Z",
     "iopub.status.idle": "2025-09-12T07:38:10.498043Z",
     "shell.execute_reply": "2025-09-12T07:38:10.497424Z"
    },
    "papermill": {
     "duration": 0.010592,
     "end_time": "2025-09-12T07:38:10.499653",
     "exception": false,
     "start_time": "2025-09-12T07:38:10.489061",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "words = [\"fairly\", \"goes\", \"ingesting\", \"eating\", \"eats\", \"eaten\", \"writing\",\n",
    "         \"writes\", \"programming\", \"programs\", \"history\", \"finally\", \"finalized\", \"sportingly\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ad6135",
   "metadata": {
    "papermill": {
     "duration": 0.00409,
     "end_time": "2025-09-12T07:38:10.508073",
     "exception": false,
     "start_time": "2025-09-12T07:38:10.503983",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d8539bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-12T07:38:10.517551Z",
     "iopub.status.busy": "2025-09-12T07:38:10.517261Z",
     "iopub.status.idle": "2025-09-12T07:38:10.522743Z",
     "shell.execute_reply": "2025-09-12T07:38:10.521755Z"
    },
    "papermill": {
     "duration": 0.011825,
     "end_time": "2025-09-12T07:38:10.524042",
     "exception": false,
     "start_time": "2025-09-12T07:38:10.512217",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fairly       ----> fairli\n",
      "goes         ----> goe\n",
      "ingesting    ----> ingest\n",
      "eating       ----> eat\n",
      "eats         ----> eat\n",
      "eaten        ----> eaten\n",
      "writing      ----> write\n",
      "writes       ----> write\n",
      "programming  ----> program\n",
      "programs     ----> program\n",
      "history      ----> histori\n",
      "finally      ----> final\n",
      "finalized    ----> final\n",
      "sportingly   ----> sportingli\n"
     ]
    }
   ],
   "source": [
    "porter = PorterStemmer()\n",
    "\n",
    "for word in words:\n",
    "    print(f\"{word:12} ----> {porter.stem(word)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eac0599",
   "metadata": {
    "papermill": {
     "duration": 0.004132,
     "end_time": "2025-09-12T07:38:10.532454",
     "exception": false,
     "start_time": "2025-09-12T07:38:10.528322",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a520868a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-12T07:38:10.542416Z",
     "iopub.status.busy": "2025-09-12T07:38:10.541662Z",
     "iopub.status.idle": "2025-09-12T07:38:10.547016Z",
     "shell.execute_reply": "2025-09-12T07:38:10.546146Z"
    },
    "papermill": {
     "duration": 0.011772,
     "end_time": "2025-09-12T07:38:10.548375",
     "exception": false,
     "start_time": "2025-09-12T07:38:10.536603",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fairly       ----> fair\n",
      "goes         ----> goe\n",
      "ingesting    ----> ingest\n",
      "eating       ----> eat\n",
      "eats         ----> eat\n",
      "eaten        ----> eaten\n",
      "writing      ----> write\n",
      "writes       ----> write\n",
      "programming  ----> program\n",
      "programs     ----> program\n",
      "history      ----> histori\n",
      "finally      ----> final\n",
      "finalized    ----> final\n",
      "sportingly   ----> sport\n"
     ]
    }
   ],
   "source": [
    "snowball = SnowballStemmer(language='english', ignore_stopwords=False)\n",
    "\n",
    "for word in words:\n",
    "    print(f\"{word:12} ----> {snowball.stem(word)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fbf4d8",
   "metadata": {
    "papermill": {
     "duration": 0.004104,
     "end_time": "2025-09-12T07:38:10.556718",
     "exception": false,
     "start_time": "2025-09-12T07:38:10.552614",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### RegexpStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5eb360b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-12T07:38:10.566000Z",
     "iopub.status.busy": "2025-09-12T07:38:10.565701Z",
     "iopub.status.idle": "2025-09-12T07:38:10.570947Z",
     "shell.execute_reply": "2025-09-12T07:38:10.569856Z"
    },
    "papermill": {
     "duration": 0.011572,
     "end_time": "2025-09-12T07:38:10.572375",
     "exception": false,
     "start_time": "2025-09-12T07:38:10.560803",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fairly       ----> fairly\n",
      "goes         ----> goe\n",
      "ingesting    ----> ingest\n",
      "eating       ----> eat\n",
      "eats         ----> eat\n",
      "eaten        ----> eaten\n",
      "writing      ----> writ\n",
      "writes       ----> write\n",
      "programming  ----> programm\n",
      "programs     ----> program\n",
      "history      ----> history\n",
      "finally      ----> finally\n",
      "finalized    ----> finalized\n",
      "sportingly   ----> sportingly\n"
     ]
    }
   ],
   "source": [
    "regexp = RegexpStemmer('ing$|s$|e$|able$', min=4)\n",
    "\n",
    "for word in words:\n",
    "    print(f\"{word:12} ----> {regexp.stem(word)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b422c6",
   "metadata": {
    "papermill": {
     "duration": 0.00421,
     "end_time": "2025-09-12T07:38:10.581161",
     "exception": false,
     "start_time": "2025-09-12T07:38:10.576951",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Lemmatization\n",
    "\n",
    "The process of reducing a word to its base or dictionary form, called a `lemma`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "694242a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-12T07:38:10.590837Z",
     "iopub.status.busy": "2025-09-12T07:38:10.590550Z",
     "iopub.status.idle": "2025-09-12T07:38:10.594280Z",
     "shell.execute_reply": "2025-09-12T07:38:10.593513Z"
    },
    "papermill": {
     "duration": 0.010266,
     "end_time": "2025-09-12T07:38:10.595691",
     "exception": false,
     "start_time": "2025-09-12T07:38:10.585425",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.stem import (\n",
    "    WordNetLemmatizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5971c44b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-12T07:38:10.606001Z",
     "iopub.status.busy": "2025-09-12T07:38:10.605433Z",
     "iopub.status.idle": "2025-09-12T07:38:10.609467Z",
     "shell.execute_reply": "2025-09-12T07:38:10.608791Z"
    },
    "papermill": {
     "duration": 0.010438,
     "end_time": "2025-09-12T07:38:10.610656",
     "exception": false,
     "start_time": "2025-09-12T07:38:10.600218",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "words = [\"fairly\", \"goes\", \"ingesting\", \"eating\", \"eats\", \"eaten\", \"writing\",\n",
    "         \"writes\", \"programming\", \"programs\", \"history\", \"finally\", \"finalized\", \"sportingly\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a65f6d9",
   "metadata": {
    "papermill": {
     "duration": 0.004163,
     "end_time": "2025-09-12T07:38:10.619179",
     "exception": false,
     "start_time": "2025-09-12T07:38:10.615016",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1bdcc630",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-12T07:38:10.629226Z",
     "iopub.status.busy": "2025-09-12T07:38:10.628667Z",
     "iopub.status.idle": "2025-09-12T07:38:13.830646Z",
     "shell.execute_reply": "2025-09-12T07:38:13.829657Z"
    },
    "papermill": {
     "duration": 3.208608,
     "end_time": "2025-09-12T07:38:13.832032",
     "exception": false,
     "start_time": "2025-09-12T07:38:10.623424",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fairly       ----> fairly\n",
      "goes         ----> go\n",
      "ingesting    ----> ingest\n",
      "eating       ----> eat\n",
      "eats         ----> eat\n",
      "eaten        ----> eat\n",
      "writing      ----> write\n",
      "writes       ----> write\n",
      "programming  ----> program\n",
      "programs     ----> program\n",
      "history      ----> history\n",
      "finally      ----> finally\n",
      "finalized    ----> finalize\n",
      "sportingly   ----> sportingly\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "\n",
    "wordnet = WordNetLemmatizer()\n",
    "\n",
    "for word in words:\n",
    "    print(f\"{word:12} ----> {wordnet.lemmatize(word=word, pos='v')}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 11.846066,
   "end_time": "2025-09-12T07:38:14.756435",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-09-12T07:38:02.910369",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
