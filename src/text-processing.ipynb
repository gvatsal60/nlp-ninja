{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdae2a3c",
   "metadata": {
    "papermill": {
     "duration": 0.005144,
     "end_time": "2025-09-12T06:45:03.772295",
     "exception": false,
     "start_time": "2025-09-12T06:45:03.767151",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# NLP - Pre-processing\n",
    "\n",
    "Corpus -> Paragraph\n",
    "\n",
    "Documents -> Sentences\n",
    "\n",
    "Vocabulary -> Unique Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3fb954",
   "metadata": {
    "papermill": {
     "duration": 0.003658,
     "end_time": "2025-09-12T06:45:03.780064",
     "exception": false,
     "start_time": "2025-09-12T06:45:03.776406",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Tokenization\n",
    "\n",
    "It is the fundamental process of breaking down a text into smaller, manageable units called `tokens`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bcd50aeb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-12T06:45:03.789641Z",
     "iopub.status.busy": "2025-09-12T06:45:03.789241Z",
     "iopub.status.idle": "2025-09-12T06:45:06.006528Z",
     "shell.execute_reply": "2025-09-12T06:45:06.005463Z"
    },
    "papermill": {
     "duration": 2.224142,
     "end_time": "2025-09-12T06:45:06.008307",
     "exception": false,
     "start_time": "2025-09-12T06:45:03.784165",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import (\n",
    "sent_tokenize,\n",
    "word_tokenize,\n",
    "wordpunct_tokenize,\n",
    "TreebankWordTokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "adaeecb1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-12T06:45:06.018062Z",
     "iopub.status.busy": "2025-09-12T06:45:06.017660Z",
     "iopub.status.idle": "2025-09-12T06:45:06.023081Z",
     "shell.execute_reply": "2025-09-12T06:45:06.021918Z"
    },
    "papermill": {
     "duration": 0.011958,
     "end_time": "2025-09-12T06:45:06.024688",
     "exception": false,
     "start_time": "2025-09-12T06:45:06.012730",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "text = \"\"\"There are multiple ways we can perform cost's tokenization on given text data.\n",
    "We can choose any method based on langauge, library and purpose of modeling.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7f55fe",
   "metadata": {
    "papermill": {
     "duration": 0.003768,
     "end_time": "2025-09-12T06:45:06.032453",
     "exception": false,
     "start_time": "2025-09-12T06:45:06.028685",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb5ec2d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-12T06:45:06.041524Z",
     "iopub.status.busy": "2025-09-12T06:45:06.040762Z",
     "iopub.status.idle": "2025-09-12T06:45:06.079257Z",
     "shell.execute_reply": "2025-09-12T06:45:06.078324Z"
    },
    "papermill": {
     "duration": 0.044635,
     "end_time": "2025-09-12T06:45:06.080823",
     "exception": false,
     "start_time": "2025-09-12T06:45:06.036188",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"There are multiple ways we can perform cost's tokenization on given text data.\",\n",
       " 'We can choose any method based on langauge, library and purpose of modeling.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = sent_tokenize(text=text, language='english')\n",
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5496355c",
   "metadata": {
    "papermill": {
     "duration": 0.004019,
     "end_time": "2025-09-12T06:45:06.089145",
     "exception": false,
     "start_time": "2025-09-12T06:45:06.085126",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5038d18",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-12T06:45:06.099322Z",
     "iopub.status.busy": "2025-09-12T06:45:06.099011Z",
     "iopub.status.idle": "2025-09-12T06:45:06.106003Z",
     "shell.execute_reply": "2025-09-12T06:45:06.105114Z"
    },
    "papermill": {
     "duration": 0.014202,
     "end_time": "2025-09-12T06:45:06.107385",
     "exception": false,
     "start_time": "2025-09-12T06:45:06.093183",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['There',\n",
       " 'are',\n",
       " 'multiple',\n",
       " 'ways',\n",
       " 'we',\n",
       " 'can',\n",
       " 'perform',\n",
       " 'cost',\n",
       " \"'s\",\n",
       " 'tokenization',\n",
       " 'on',\n",
       " 'given',\n",
       " 'text',\n",
       " 'data',\n",
       " '.',\n",
       " 'We',\n",
       " 'can',\n",
       " 'choose',\n",
       " 'any',\n",
       " 'method',\n",
       " 'based',\n",
       " 'on',\n",
       " 'langauge',\n",
       " ',',\n",
       " 'library',\n",
       " 'and',\n",
       " 'purpose',\n",
       " 'of',\n",
       " 'modeling',\n",
       " '.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = word_tokenize(text=text, language='english')\n",
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d15784",
   "metadata": {
    "papermill": {
     "duration": 0.00384,
     "end_time": "2025-09-12T06:45:06.115656",
     "exception": false,
     "start_time": "2025-09-12T06:45:06.111816",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### wordpunct_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4032095c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-12T06:45:06.125629Z",
     "iopub.status.busy": "2025-09-12T06:45:06.125266Z",
     "iopub.status.idle": "2025-09-12T06:45:06.132007Z",
     "shell.execute_reply": "2025-09-12T06:45:06.131155Z"
    },
    "papermill": {
     "duration": 0.013323,
     "end_time": "2025-09-12T06:45:06.133427",
     "exception": false,
     "start_time": "2025-09-12T06:45:06.120104",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['There',\n",
       " 'are',\n",
       " 'multiple',\n",
       " 'ways',\n",
       " 'we',\n",
       " 'can',\n",
       " 'perform',\n",
       " 'cost',\n",
       " \"'\",\n",
       " 's',\n",
       " 'tokenization',\n",
       " 'on',\n",
       " 'given',\n",
       " 'text',\n",
       " 'data',\n",
       " '.',\n",
       " 'We',\n",
       " 'can',\n",
       " 'choose',\n",
       " 'any',\n",
       " 'method',\n",
       " 'based',\n",
       " 'on',\n",
       " 'langauge',\n",
       " ',',\n",
       " 'library',\n",
       " 'and',\n",
       " 'purpose',\n",
       " 'of',\n",
       " 'modeling',\n",
       " '.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordpunct = wordpunct_tokenize(text=text)\n",
    "wordpunct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395d1c01",
   "metadata": {
    "papermill": {
     "duration": 0.00491,
     "end_time": "2025-09-12T06:45:06.143282",
     "exception": false,
     "start_time": "2025-09-12T06:45:06.138372",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5dc1e384",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-12T06:45:06.153591Z",
     "iopub.status.busy": "2025-09-12T06:45:06.153277Z",
     "iopub.status.idle": "2025-09-12T06:45:06.159254Z",
     "shell.execute_reply": "2025-09-12T06:45:06.158420Z"
    },
    "papermill": {
     "duration": 0.012755,
     "end_time": "2025-09-12T06:45:06.160772",
     "exception": false,
     "start_time": "2025-09-12T06:45:06.148017",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['There',\n",
       " 'are',\n",
       " 'multiple',\n",
       " 'ways',\n",
       " 'we',\n",
       " 'can',\n",
       " 'perform',\n",
       " 'cost',\n",
       " \"'s\",\n",
       " 'tokenization',\n",
       " 'on',\n",
       " 'given',\n",
       " 'text',\n",
       " 'data.',\n",
       " 'We',\n",
       " 'can',\n",
       " 'choose',\n",
       " 'any',\n",
       " 'method',\n",
       " 'based',\n",
       " 'on',\n",
       " 'langauge',\n",
       " ',',\n",
       " 'library',\n",
       " 'and',\n",
       " 'purpose',\n",
       " 'of',\n",
       " 'modeling',\n",
       " '.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "treebank = TreebankWordTokenizer().tokenize(text)\n",
    "treebank"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db03f35",
   "metadata": {
    "papermill": {
     "duration": 0.004368,
     "end_time": "2025-09-12T06:45:06.169945",
     "exception": false,
     "start_time": "2025-09-12T06:45:06.165577",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Stemming\n",
    "\n",
    "It is the process of reducing a word to its base form, known as the `stem`. This `stem` may **not** be a valid word in itself, but it serves as the foundation to which prefixes and suffixes are attached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6fdacc0b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-12T06:45:06.180061Z",
     "iopub.status.busy": "2025-09-12T06:45:06.179748Z",
     "iopub.status.idle": "2025-09-12T06:45:06.183626Z",
     "shell.execute_reply": "2025-09-12T06:45:06.182926Z"
    },
    "papermill": {
     "duration": 0.010935,
     "end_time": "2025-09-12T06:45:06.185212",
     "exception": false,
     "start_time": "2025-09-12T06:45:06.174277",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.stem import (\n",
    "PorterStemmer,\n",
    "SnowballStemmer,\n",
    "RegexpStemmer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9684b781",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-12T06:45:06.195800Z",
     "iopub.status.busy": "2025-09-12T06:45:06.195094Z",
     "iopub.status.idle": "2025-09-12T06:45:06.199854Z",
     "shell.execute_reply": "2025-09-12T06:45:06.199098Z"
    },
    "papermill": {
     "duration": 0.011922,
     "end_time": "2025-09-12T06:45:06.201536",
     "exception": false,
     "start_time": "2025-09-12T06:45:06.189614",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "words=[\"fairly\",\"goes\",\"ingeating\",\"eating\",\"eats\",\"eaten\",\"writing\",\"writes\",\"programming\",\"programs\",\"history\",\"finally\",\"finalized\", \"sportingly\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13ddc7d",
   "metadata": {
    "papermill": {
     "duration": 0.004088,
     "end_time": "2025-09-12T06:45:06.210141",
     "exception": false,
     "start_time": "2025-09-12T06:45:06.206053",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c197995",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-12T06:45:06.219746Z",
     "iopub.status.busy": "2025-09-12T06:45:06.219473Z",
     "iopub.status.idle": "2025-09-12T06:45:06.225636Z",
     "shell.execute_reply": "2025-09-12T06:45:06.224622Z"
    },
    "papermill": {
     "duration": 0.012855,
     "end_time": "2025-09-12T06:45:06.227187",
     "exception": false,
     "start_time": "2025-09-12T06:45:06.214332",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fairly       ----> fairli\n",
      "goes         ----> goe\n",
      "ingeating    ----> ingeat\n",
      "eating       ----> eat\n",
      "eats         ----> eat\n",
      "eaten        ----> eaten\n",
      "writing      ----> write\n",
      "writes       ----> write\n",
      "programming  ----> program\n",
      "programs     ----> program\n",
      "history      ----> histori\n",
      "finally      ----> final\n",
      "finalized    ----> final\n",
      "sportingly   ----> sportingli\n"
     ]
    }
   ],
   "source": [
    "porter = PorterStemmer()\n",
    "\n",
    "for word in words:\n",
    "    print(f\"{word:12} ----> {porter.stem(word)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d45763a",
   "metadata": {
    "papermill": {
     "duration": 0.004023,
     "end_time": "2025-09-12T06:45:06.235638",
     "exception": false,
     "start_time": "2025-09-12T06:45:06.231615",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3010a1eb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-12T06:45:06.245811Z",
     "iopub.status.busy": "2025-09-12T06:45:06.245098Z",
     "iopub.status.idle": "2025-09-12T06:45:06.250881Z",
     "shell.execute_reply": "2025-09-12T06:45:06.249586Z"
    },
    "papermill": {
     "duration": 0.012412,
     "end_time": "2025-09-12T06:45:06.252280",
     "exception": false,
     "start_time": "2025-09-12T06:45:06.239868",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fairly       ----> fair\n",
      "goes         ----> goe\n",
      "ingeating    ----> ingeat\n",
      "eating       ----> eat\n",
      "eats         ----> eat\n",
      "eaten        ----> eaten\n",
      "writing      ----> write\n",
      "writes       ----> write\n",
      "programming  ----> program\n",
      "programs     ----> program\n",
      "history      ----> histori\n",
      "finally      ----> final\n",
      "finalized    ----> final\n",
      "sportingly   ----> sport\n"
     ]
    }
   ],
   "source": [
    "snowball = SnowballStemmer(language='english', ignore_stopwords=False)\n",
    "\n",
    "for word in words:\n",
    "    print(f\"{word:12} ----> {snowball.stem(word)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8506251",
   "metadata": {
    "papermill": {
     "duration": 0.004181,
     "end_time": "2025-09-12T06:45:06.261025",
     "exception": false,
     "start_time": "2025-09-12T06:45:06.256844",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### RegexpStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d56cdf1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-12T06:45:06.271872Z",
     "iopub.status.busy": "2025-09-12T06:45:06.271565Z",
     "iopub.status.idle": "2025-09-12T06:45:06.276796Z",
     "shell.execute_reply": "2025-09-12T06:45:06.275804Z"
    },
    "papermill": {
     "duration": 0.012618,
     "end_time": "2025-09-12T06:45:06.278082",
     "exception": false,
     "start_time": "2025-09-12T06:45:06.265464",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fairly       ----> fairly\n",
      "goes         ----> goe\n",
      "ingeating    ----> ingeat\n",
      "eating       ----> eat\n",
      "eats         ----> eat\n",
      "eaten        ----> eaten\n",
      "writing      ----> writ\n",
      "writes       ----> write\n",
      "programming  ----> programm\n",
      "programs     ----> program\n",
      "history      ----> history\n",
      "finally      ----> finally\n",
      "finalized    ----> finalized\n",
      "sportingly   ----> sportingly\n"
     ]
    }
   ],
   "source": [
    "regexp=RegexpStemmer('ing$|s$|e$|able$', min=4)\n",
    "\n",
    "for word in words:\n",
    "    print(f\"{word:12} ----> {regexp.stem(word)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64cf576",
   "metadata": {
    "papermill": {
     "duration": 0.004312,
     "end_time": "2025-09-12T06:45:06.287778",
     "exception": false,
     "start_time": "2025-09-12T06:45:06.283466",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Lemmatization\n",
    "\n",
    "The process of reducing a word to its base or dictionary form, called a `lemma`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4020064b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-12T06:45:06.298429Z",
     "iopub.status.busy": "2025-09-12T06:45:06.298130Z",
     "iopub.status.idle": "2025-09-12T06:45:06.302376Z",
     "shell.execute_reply": "2025-09-12T06:45:06.301267Z"
    },
    "papermill": {
     "duration": 0.011521,
     "end_time": "2025-09-12T06:45:06.304164",
     "exception": false,
     "start_time": "2025-09-12T06:45:06.292643",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.stem import (\n",
    "WordNetLemmatizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6bc203a2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-12T06:45:06.315634Z",
     "iopub.status.busy": "2025-09-12T06:45:06.315332Z",
     "iopub.status.idle": "2025-09-12T06:45:06.319658Z",
     "shell.execute_reply": "2025-09-12T06:45:06.318814Z"
    },
    "papermill": {
     "duration": 0.011306,
     "end_time": "2025-09-12T06:45:06.320981",
     "exception": false,
     "start_time": "2025-09-12T06:45:06.309675",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "words=[\"fairly\",\"goes\",\"ingeating\",\"eating\",\"eats\",\"eaten\",\"writing\",\"writes\",\"programming\",\"programs\",\"history\",\"finally\",\"finalized\", \"sportingly\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5033927",
   "metadata": {
    "papermill": {
     "duration": 0.004129,
     "end_time": "2025-09-12T06:45:06.329800",
     "exception": false,
     "start_time": "2025-09-12T06:45:06.325671",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9cd28c44",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-12T06:45:06.340340Z",
     "iopub.status.busy": "2025-09-12T06:45:06.339498Z",
     "iopub.status.idle": "2025-09-12T06:45:09.492980Z",
     "shell.execute_reply": "2025-09-12T06:45:09.491804Z"
    },
    "papermill": {
     "duration": 3.160428,
     "end_time": "2025-09-12T06:45:09.494680",
     "exception": false,
     "start_time": "2025-09-12T06:45:06.334252",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fairly       ----> fairly\n",
      "goes         ----> go\n",
      "ingeating    ----> ingeating\n",
      "eating       ----> eat\n",
      "eats         ----> eat\n",
      "eaten        ----> eat\n",
      "writing      ----> write\n",
      "writes       ----> write\n",
      "programming  ----> program\n",
      "programs     ----> program\n",
      "history      ----> history\n",
      "finally      ----> finally\n",
      "finalized    ----> finalize\n",
      "sportingly   ----> sportingly\n"
     ]
    }
   ],
   "source": [
    "wordnet=WordNetLemmatizer()\n",
    "\n",
    "for word in words:\n",
    "    print(f\"{word:12} ----> {wordnet.lemmatize(word=word, pos='v')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b73bea",
   "metadata": {
    "papermill": {
     "duration": 0.00456,
     "end_time": "2025-09-12T06:45:09.506019",
     "exception": false,
     "start_time": "2025-09-12T06:45:09.501459",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 11.984333,
   "end_time": "2025-09-12T06:45:10.531455",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-09-12T06:44:58.547122",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
