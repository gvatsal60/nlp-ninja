{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d176b70a",
   "metadata": {
    "papermill": {
     "duration": 0.005877,
     "end_time": "2025-09-12T10:11:49.063817",
     "exception": false,
     "start_time": "2025-09-12T10:11:49.057940",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# NLP - Pre-processing\n",
    "\n",
    "Corpus -> Paragraph\n",
    "\n",
    "Documents -> Sentences\n",
    "\n",
    "Vocabulary -> Unique Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bc2f956",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-12T10:11:49.075339Z",
     "iopub.status.busy": "2025-09-12T10:11:49.075047Z",
     "iopub.status.idle": "2025-09-12T10:11:51.349927Z",
     "shell.execute_reply": "2025-09-12T10:11:51.349009Z"
    },
    "papermill": {
     "duration": 2.28262,
     "end_time": "2025-09-12T10:11:51.351717",
     "exception": false,
     "start_time": "2025-09-12T10:11:49.069097",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25fc00f1",
   "metadata": {
    "papermill": {
     "duration": 0.004556,
     "end_time": "2025-09-12T10:11:51.361299",
     "exception": false,
     "start_time": "2025-09-12T10:11:51.356743",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Tokenization\n",
    "\n",
    "It is the fundamental process of breaking down a text into smaller, manageable units called `tokens`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c02ca72",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-12T10:11:51.372672Z",
     "iopub.status.busy": "2025-09-12T10:11:51.371834Z",
     "iopub.status.idle": "2025-09-12T10:11:51.376310Z",
     "shell.execute_reply": "2025-09-12T10:11:51.375418Z"
    },
    "papermill": {
     "duration": 0.011818,
     "end_time": "2025-09-12T10:11:51.377879",
     "exception": false,
     "start_time": "2025-09-12T10:11:51.366061",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import (\n",
    "    sent_tokenize,\n",
    "    word_tokenize,\n",
    "    wordpunct_tokenize,\n",
    "    TreebankWordTokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ef549b6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-12T10:11:51.389353Z",
     "iopub.status.busy": "2025-09-12T10:11:51.389031Z",
     "iopub.status.idle": "2025-09-12T10:11:51.393247Z",
     "shell.execute_reply": "2025-09-12T10:11:51.392488Z"
    },
    "papermill": {
     "duration": 0.011959,
     "end_time": "2025-09-12T10:11:51.394822",
     "exception": false,
     "start_time": "2025-09-12T10:11:51.382863",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "text = \"\"\"There are multiple ways we can perform cost's tokenization on given text data.\n",
    "We can choose any method based on language, library and purpose of modeling.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc1ab79",
   "metadata": {
    "papermill": {
     "duration": 0.005324,
     "end_time": "2025-09-12T10:11:51.405014",
     "exception": false,
     "start_time": "2025-09-12T10:11:51.399690",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7109042a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-12T10:11:51.416110Z",
     "iopub.status.busy": "2025-09-12T10:11:51.415762Z",
     "iopub.status.idle": "2025-09-12T10:11:51.456356Z",
     "shell.execute_reply": "2025-09-12T10:11:51.455343Z"
    },
    "papermill": {
     "duration": 0.048319,
     "end_time": "2025-09-12T10:11:51.457963",
     "exception": false,
     "start_time": "2025-09-12T10:11:51.409644",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"There are multiple ways we can perform cost's tokenization on given text data.\",\n",
       " 'We can choose any method based on language, library and purpose of modeling.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = sent_tokenize(text=text, language=\"english\")\n",
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dfac2d5",
   "metadata": {
    "papermill": {
     "duration": 0.00477,
     "end_time": "2025-09-12T10:11:51.467861",
     "exception": false,
     "start_time": "2025-09-12T10:11:51.463091",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eec48b83",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-12T10:11:51.479359Z",
     "iopub.status.busy": "2025-09-12T10:11:51.479059Z",
     "iopub.status.idle": "2025-09-12T10:11:51.485892Z",
     "shell.execute_reply": "2025-09-12T10:11:51.484959Z"
    },
    "papermill": {
     "duration": 0.014432,
     "end_time": "2025-09-12T10:11:51.487246",
     "exception": false,
     "start_time": "2025-09-12T10:11:51.472814",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['There',\n",
       " 'are',\n",
       " 'multiple',\n",
       " 'ways',\n",
       " 'we',\n",
       " 'can',\n",
       " 'perform',\n",
       " 'cost',\n",
       " \"'s\",\n",
       " 'tokenization',\n",
       " 'on',\n",
       " 'given',\n",
       " 'text',\n",
       " 'data',\n",
       " '.',\n",
       " 'We',\n",
       " 'can',\n",
       " 'choose',\n",
       " 'any',\n",
       " 'method',\n",
       " 'based',\n",
       " 'on',\n",
       " 'language',\n",
       " ',',\n",
       " 'library',\n",
       " 'and',\n",
       " 'purpose',\n",
       " 'of',\n",
       " 'modeling',\n",
       " '.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = word_tokenize(text=text, language=\"english\")\n",
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f586e98f",
   "metadata": {
    "papermill": {
     "duration": 0.004822,
     "end_time": "2025-09-12T10:11:51.497357",
     "exception": false,
     "start_time": "2025-09-12T10:11:51.492535",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### wordpunct_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fab56742",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-12T10:11:51.508484Z",
     "iopub.status.busy": "2025-09-12T10:11:51.508157Z",
     "iopub.status.idle": "2025-09-12T10:11:51.738625Z",
     "shell.execute_reply": "2025-09-12T10:11:51.737708Z"
    },
    "papermill": {
     "duration": 0.237903,
     "end_time": "2025-09-12T10:11:51.740201",
     "exception": false,
     "start_time": "2025-09-12T10:11:51.502298",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['There',\n",
       " 'are',\n",
       " 'multiple',\n",
       " 'ways',\n",
       " 'we',\n",
       " 'can',\n",
       " 'perform',\n",
       " 'cost',\n",
       " \"'\",\n",
       " 's',\n",
       " 'tokenization',\n",
       " 'on',\n",
       " 'given',\n",
       " 'text',\n",
       " 'data',\n",
       " '.',\n",
       " 'We',\n",
       " 'can',\n",
       " 'choose',\n",
       " 'any',\n",
       " 'method',\n",
       " 'based',\n",
       " 'on',\n",
       " 'language',\n",
       " ',',\n",
       " 'library',\n",
       " 'and',\n",
       " 'purpose',\n",
       " 'of',\n",
       " 'modeling',\n",
       " '.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"punkt\")\n",
    "\n",
    "wordpunct = wordpunct_tokenize(text=text)\n",
    "wordpunct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca00160c",
   "metadata": {
    "papermill": {
     "duration": 0.005,
     "end_time": "2025-09-12T10:11:51.750644",
     "exception": false,
     "start_time": "2025-09-12T10:11:51.745644",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f66e6f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-12T10:11:51.762699Z",
     "iopub.status.busy": "2025-09-12T10:11:51.762342Z",
     "iopub.status.idle": "2025-09-12T10:11:51.768611Z",
     "shell.execute_reply": "2025-09-12T10:11:51.767750Z"
    },
    "papermill": {
     "duration": 0.013815,
     "end_time": "2025-09-12T10:11:51.769985",
     "exception": false,
     "start_time": "2025-09-12T10:11:51.756170",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['There',\n",
       " 'are',\n",
       " 'multiple',\n",
       " 'ways',\n",
       " 'we',\n",
       " 'can',\n",
       " 'perform',\n",
       " 'cost',\n",
       " \"'s\",\n",
       " 'tokenization',\n",
       " 'on',\n",
       " 'given',\n",
       " 'text',\n",
       " 'data.',\n",
       " 'We',\n",
       " 'can',\n",
       " 'choose',\n",
       " 'any',\n",
       " 'method',\n",
       " 'based',\n",
       " 'on',\n",
       " 'language',\n",
       " ',',\n",
       " 'library',\n",
       " 'and',\n",
       " 'purpose',\n",
       " 'of',\n",
       " 'modeling',\n",
       " '.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "treebank = TreebankWordTokenizer().tokenize(text)\n",
    "treebank"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb7c8c6",
   "metadata": {
    "papermill": {
     "duration": 0.00521,
     "end_time": "2025-09-12T10:11:51.780612",
     "exception": false,
     "start_time": "2025-09-12T10:11:51.775402",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Stemming\n",
    "\n",
    "It is the process of reducing a word to its base form, known as the `stem`. This `stem` may **not** be a valid word in itself, but it serves as the foundation to which prefixes and suffixes are attached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c0de01d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-12T10:11:51.792935Z",
     "iopub.status.busy": "2025-09-12T10:11:51.792620Z",
     "iopub.status.idle": "2025-09-12T10:11:51.797084Z",
     "shell.execute_reply": "2025-09-12T10:11:51.796268Z"
    },
    "papermill": {
     "duration": 0.012387,
     "end_time": "2025-09-12T10:11:51.798443",
     "exception": false,
     "start_time": "2025-09-12T10:11:51.786056",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer, SnowballStemmer, RegexpStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb5655cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-12T10:11:51.810990Z",
     "iopub.status.busy": "2025-09-12T10:11:51.810158Z",
     "iopub.status.idle": "2025-09-12T10:11:51.814738Z",
     "shell.execute_reply": "2025-09-12T10:11:51.813799Z"
    },
    "papermill": {
     "duration": 0.012197,
     "end_time": "2025-09-12T10:11:51.816148",
     "exception": false,
     "start_time": "2025-09-12T10:11:51.803951",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "words = [\n",
    "    \"fairly\",\n",
    "    \"goes\",\n",
    "    \"ingesting\",\n",
    "    \"eating\",\n",
    "    \"eats\",\n",
    "    \"eaten\",\n",
    "    \"writing\",\n",
    "    \"writes\",\n",
    "    \"programming\",\n",
    "    \"programs\",\n",
    "    \"history\",\n",
    "    \"finally\",\n",
    "    \"finalized\",\n",
    "    \"sportingly\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa4f32b",
   "metadata": {
    "papermill": {
     "duration": 0.005203,
     "end_time": "2025-09-12T10:11:51.826714",
     "exception": false,
     "start_time": "2025-09-12T10:11:51.821511",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3eb2d31d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-12T10:11:51.838202Z",
     "iopub.status.busy": "2025-09-12T10:11:51.837916Z",
     "iopub.status.idle": "2025-09-12T10:11:51.843528Z",
     "shell.execute_reply": "2025-09-12T10:11:51.842373Z"
    },
    "papermill": {
     "duration": 0.013214,
     "end_time": "2025-09-12T10:11:51.845080",
     "exception": false,
     "start_time": "2025-09-12T10:11:51.831866",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fairly       ----> fairli\n",
      "goes         ----> goe\n",
      "ingesting    ----> ingest\n",
      "eating       ----> eat\n",
      "eats         ----> eat\n",
      "eaten        ----> eaten\n",
      "writing      ----> write\n",
      "writes       ----> write\n",
      "programming  ----> program\n",
      "programs     ----> program\n",
      "history      ----> histori\n",
      "finally      ----> final\n",
      "finalized    ----> final\n",
      "sportingly   ----> sportingli\n"
     ]
    }
   ],
   "source": [
    "porter = PorterStemmer()\n",
    "\n",
    "for word in words:\n",
    "    print(f\"{word:12} ----> {porter.stem(word)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f98ad4d",
   "metadata": {
    "papermill": {
     "duration": 0.005139,
     "end_time": "2025-09-12T10:11:51.855749",
     "exception": false,
     "start_time": "2025-09-12T10:11:51.850610",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9fe8a9c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-12T10:11:51.868653Z",
     "iopub.status.busy": "2025-09-12T10:11:51.867935Z",
     "iopub.status.idle": "2025-09-12T10:11:51.873095Z",
     "shell.execute_reply": "2025-09-12T10:11:51.872276Z"
    },
    "papermill": {
     "duration": 0.012964,
     "end_time": "2025-09-12T10:11:51.874497",
     "exception": false,
     "start_time": "2025-09-12T10:11:51.861533",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fairly       ----> fair\n",
      "goes         ----> goe\n",
      "ingesting    ----> ingest\n",
      "eating       ----> eat\n",
      "eats         ----> eat\n",
      "eaten        ----> eaten\n",
      "writing      ----> write\n",
      "writes       ----> write\n",
      "programming  ----> program\n",
      "programs     ----> program\n",
      "history      ----> histori\n",
      "finally      ----> final\n",
      "finalized    ----> final\n",
      "sportingly   ----> sport\n"
     ]
    }
   ],
   "source": [
    "snowball = SnowballStemmer(language=\"english\", ignore_stopwords=False)\n",
    "\n",
    "for word in words:\n",
    "    print(f\"{word:12} ----> {snowball.stem(word)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed57fc06",
   "metadata": {
    "papermill": {
     "duration": 0.005164,
     "end_time": "2025-09-12T10:11:51.885073",
     "exception": false,
     "start_time": "2025-09-12T10:11:51.879909",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### RegexpStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e0f116b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-12T10:11:51.896997Z",
     "iopub.status.busy": "2025-09-12T10:11:51.896686Z",
     "iopub.status.idle": "2025-09-12T10:11:51.902213Z",
     "shell.execute_reply": "2025-09-12T10:11:51.901168Z"
    },
    "papermill": {
     "duration": 0.01339,
     "end_time": "2025-09-12T10:11:51.903781",
     "exception": false,
     "start_time": "2025-09-12T10:11:51.890391",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fairly       ----> fairly\n",
      "goes         ----> goe\n",
      "ingesting    ----> ingest\n",
      "eating       ----> eat\n",
      "eats         ----> eat\n",
      "eaten        ----> eaten\n",
      "writing      ----> writ\n",
      "writes       ----> write\n",
      "programming  ----> programm\n",
      "programs     ----> program\n",
      "history      ----> history\n",
      "finally      ----> finally\n",
      "finalized    ----> finalized\n",
      "sportingly   ----> sportingly\n"
     ]
    }
   ],
   "source": [
    "regexp = RegexpStemmer(\"ing$|s$|e$|able$\", min=4)\n",
    "\n",
    "for word in words:\n",
    "    print(f\"{word:12} ----> {regexp.stem(word)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3df908",
   "metadata": {
    "papermill": {
     "duration": 0.005422,
     "end_time": "2025-09-12T10:11:51.914783",
     "exception": false,
     "start_time": "2025-09-12T10:11:51.909361",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Lemmatization\n",
    "\n",
    "The process of reducing a word to its base or dictionary form, called a `lemma`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0fee526a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-12T10:11:51.927191Z",
     "iopub.status.busy": "2025-09-12T10:11:51.926385Z",
     "iopub.status.idle": "2025-09-12T10:11:51.930406Z",
     "shell.execute_reply": "2025-09-12T10:11:51.929606Z"
    },
    "papermill": {
     "duration": 0.011745,
     "end_time": "2025-09-12T10:11:51.931906",
     "exception": false,
     "start_time": "2025-09-12T10:11:51.920161",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7813fe30",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-12T10:11:51.944604Z",
     "iopub.status.busy": "2025-09-12T10:11:51.943784Z",
     "iopub.status.idle": "2025-09-12T10:11:51.948267Z",
     "shell.execute_reply": "2025-09-12T10:11:51.947377Z"
    },
    "papermill": {
     "duration": 0.01218,
     "end_time": "2025-09-12T10:11:51.949734",
     "exception": false,
     "start_time": "2025-09-12T10:11:51.937554",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "words = [\n",
    "    \"fairly\",\n",
    "    \"goes\",\n",
    "    \"ingesting\",\n",
    "    \"eating\",\n",
    "    \"eats\",\n",
    "    \"eaten\",\n",
    "    \"writing\",\n",
    "    \"writes\",\n",
    "    \"programming\",\n",
    "    \"programs\",\n",
    "    \"history\",\n",
    "    \"finally\",\n",
    "    \"finalized\",\n",
    "    \"sportingly\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13cc357",
   "metadata": {
    "papermill": {
     "duration": 0.005312,
     "end_time": "2025-09-12T10:11:51.960605",
     "exception": false,
     "start_time": "2025-09-12T10:11:51.955293",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "09b630dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-12T10:11:51.973039Z",
     "iopub.status.busy": "2025-09-12T10:11:51.972113Z",
     "iopub.status.idle": "2025-09-12T10:11:55.166424Z",
     "shell.execute_reply": "2025-09-12T10:11:55.165371Z"
    },
    "papermill": {
     "duration": 3.201916,
     "end_time": "2025-09-12T10:11:55.167903",
     "exception": false,
     "start_time": "2025-09-12T10:11:51.965987",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fairly       ----> fairly\n",
      "goes         ----> go\n",
      "ingesting    ----> ingest\n",
      "eating       ----> eat\n",
      "eats         ----> eat\n",
      "eaten        ----> eat\n",
      "writing      ----> write\n",
      "writes       ----> write\n",
      "programming  ----> program\n",
      "programs     ----> program\n",
      "history      ----> history\n",
      "finally      ----> finally\n",
      "finalized    ----> finalize\n",
      "sportingly   ----> sportingly\n"
     ]
    }
   ],
   "source": [
    "nltk.download(\"wordnet\")\n",
    "\n",
    "wordnet = WordNetLemmatizer()\n",
    "\n",
    "for word in words:\n",
    "    print(f\"{word:12} ----> {wordnet.lemmatize(word=word, pos='v')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fedc53b",
   "metadata": {
    "papermill": {
     "duration": 0.005154,
     "end_time": "2025-09-12T10:11:55.178938",
     "exception": false,
     "start_time": "2025-09-12T10:11:55.173784",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Stop Words\n",
    "\n",
    "Stop words are commonly occurring words in a language (like \"the\", \"a\", \"is\", \"in\", \"on\", \"and\") that often carry little to no semantic value for many Natural Language Processing (NLP) tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cefbe4cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-12T10:11:55.191594Z",
     "iopub.status.busy": "2025-09-12T10:11:55.190747Z",
     "iopub.status.idle": "2025-09-12T10:11:55.194768Z",
     "shell.execute_reply": "2025-09-12T10:11:55.194000Z"
    },
    "papermill": {
     "duration": 0.011771,
     "end_time": "2025-09-12T10:11:55.196064",
     "exception": false,
     "start_time": "2025-09-12T10:11:55.184293",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cfa2f272",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-12T10:11:55.208428Z",
     "iopub.status.busy": "2025-09-12T10:11:55.208097Z",
     "iopub.status.idle": "2025-09-12T10:11:55.213071Z",
     "shell.execute_reply": "2025-09-12T10:11:55.212285Z"
    },
    "papermill": {
     "duration": 0.012795,
     "end_time": "2025-09-12T10:11:55.214398",
     "exception": false,
     "start_time": "2025-09-12T10:11:55.201603",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "paragraph = \"\"\"I have three visions for India. In 3000 years of our history, people from all over \n",
    "               the world have come and invaded us, captured our lands, conquered our minds. \n",
    "               From Alexander onwards, the Greeks, the Turks, the Moguls, the Portuguese, the British,\n",
    "               the French, the Dutch, all of them came and looted us, took over what was ours. \n",
    "               Yet we have not done this to any other nation. We have not conquered anyone. \n",
    "               We have not grabbed their land, their culture, \n",
    "               their history and tried to enforce our way of life on them. \n",
    "               Why? Because we respect the freedom of others.That is why my \n",
    "               first vision is that of freedom. I believe that India got its first vision of \n",
    "               this in 1857, when we started the War of Independence. It is this freedom that\n",
    "               we must protect and nurture and build on. If we are not free, no one will respect us.\n",
    "               My second vision for India’s development. For fifty years we have been a developing nation.\n",
    "               It is time we see ourselves as a developed nation. We are among the top 5 nations of the world\n",
    "               in terms of GDP. We have a 10 percent growth rate in most areas. Our poverty levels are falling.\n",
    "               Our achievements are being globally recognised today. Yet we lack the self-confidence to\n",
    "               see ourselves as a developed nation, self-reliant and self-assured. Isn’t this incorrect?\n",
    "               I have a third vision. India must stand up to the world. Because I believe that unless India \n",
    "               stands up to the world, no one will respect us. Only strength respects strength. We must be \n",
    "               strong not only as a military power but also as an economic power. Both must go hand-in-hand. \n",
    "               My good fortune was to have worked with three great minds. Dr. Vikram Sarabhai of the Dept. of \n",
    "               space, Professor Satish Dhawan, who succeeded him and Dr. Brahm Prakash, father of nuclear material.\n",
    "               I was lucky to have worked with all three of them closely and consider this the great opportunity of my life. \n",
    "               I see four milestones in my career\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c3a3e57f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-12T10:11:55.226789Z",
     "iopub.status.busy": "2025-09-12T10:11:55.226435Z",
     "iopub.status.idle": "2025-09-12T10:11:55.242614Z",
     "shell.execute_reply": "2025-09-12T10:11:55.241666Z"
    },
    "papermill": {
     "duration": 0.024105,
     "end_time": "2025-09-12T10:11:55.244201",
     "exception": false,
     "start_time": "2025-09-12T10:11:55.220096",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I three visions India .', 'In 3000 years history , people world come invaded us , captured lands , conquered minds .', 'From Alexander onwards , Greeks , Turks , Moguls , Portuguese , British , French , Dutch , came looted us , took .', 'Yet done nation .', 'We conquered anyone .', 'We grabbed land , culture , history tried enforce way life .', 'Why ?', 'Because respect freedom others.That first vision freedom .', 'I believe India got first vision 1857 , started War Independence .', 'It freedom must protect nurture build .', 'If free , one respect us .', 'My second vision India ’ development .', 'For fifty years developing nation .', 'It time see developed nation .', 'We among top 5 nations world terms GDP .', 'We 10 percent growth rate areas .', 'Our poverty levels falling .', 'Our achievements globally recognised today .', 'Yet lack self-confidence see developed nation , self-reliant self-assured .', 'Isn ’ incorrect ?', 'I third vision .', 'India must stand world .', 'Because I believe unless India stands world , one respect us .', 'Only strength respects strength .', 'We must strong military power also economic power .', 'Both must go hand-in-hand .', 'My good fortune worked three great minds .', 'Dr. Vikram Sarabhai Dept .', 'space , Professor Satish Dhawan , succeeded Dr. Brahm Prakash , father nuclear material .', 'I lucky worked three closely consider great opportunity life .', 'I see four milestones career']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "\n",
    "# Remove stopwords from paragraph\n",
    "paragraph_tokens = sent_tokenize(paragraph)\n",
    "\n",
    "without_stop_word_sentences = []\n",
    "for sentence in paragraph_tokens:\n",
    "    word_tokens = word_tokenize(sentence)\n",
    "    words = [word for word in word_tokens if word not in stopwords]\n",
    "    sent = \" \".join(words)\n",
    "    without_stop_word_sentences.append(sent)\n",
    "\n",
    "print(without_stop_word_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc37a93",
   "metadata": {
    "papermill": {
     "duration": 0.005506,
     "end_time": "2025-09-12T10:11:55.255524",
     "exception": false,
     "start_time": "2025-09-12T10:11:55.250018",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 12.227897,
   "end_time": "2025-09-12T10:11:56.181833",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-09-12T10:11:43.953936",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
