{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "papermill": {
     "duration": 0.004821,
     "end_time": "2025-09-14T07:41:03.916069",
     "exception": false,
     "start_time": "2025-09-14T07:41:03.911248",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# NLP - Pre-processing\n",
    "\n",
    "Corpus -> Paragraph\n",
    "\n",
    "Documents -> Sentences\n",
    "\n",
    "Vocabulary -> Unique Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {
    "papermill": {
     "duration": 2.051446,
     "end_time": "2025-09-14T07:41:05.971623",
     "exception": false,
     "start_time": "2025-09-14T07:41:03.920177",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {
    "papermill": {
     "duration": 0.004106,
     "end_time": "2025-09-14T07:41:05.980047",
     "exception": false,
     "start_time": "2025-09-14T07:41:05.975941",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Tokenization\n",
    "\n",
    "It is the fundamental process of breaking down a text into smaller, manageable units called `tokens`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {
    "papermill": {
     "duration": 0.009913,
     "end_time": "2025-09-14T07:41:05.993944",
     "exception": false,
     "start_time": "2025-09-14T07:41:05.984031",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import (\n",
    "    sent_tokenize,\n",
    "    word_tokenize,\n",
    "    wordpunct_tokenize,\n",
    "    TreebankWordTokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {
    "papermill": {
     "duration": 0.010026,
     "end_time": "2025-09-14T07:41:06.008080",
     "exception": false,
     "start_time": "2025-09-14T07:41:05.998054",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "text = \"\"\"There are multiple ways we can perform cost's tokenization on given text data.\n",
    "We can choose any method based on language, library and purpose of modeling.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {
    "papermill": {
     "duration": 0.004637,
     "end_time": "2025-09-14T07:41:06.016838",
     "exception": false,
     "start_time": "2025-09-14T07:41:06.012201",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {
    "papermill": {
     "duration": 0.153874,
     "end_time": "2025-09-14T07:41:06.174571",
     "exception": false,
     "start_time": "2025-09-14T07:41:06.020697",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "nltk.download(\"punkt_tab\")\n",
    "\n",
    "sentences = sent_tokenize(text=text, language=\"english\")\n",
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {
    "papermill": {
     "duration": 0.004025,
     "end_time": "2025-09-14T07:41:06.183019",
     "exception": false,
     "start_time": "2025-09-14T07:41:06.178994",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {
    "papermill": {
     "duration": 0.012407,
     "end_time": "2025-09-14T07:41:06.199700",
     "exception": false,
     "start_time": "2025-09-14T07:41:06.187293",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "words = word_tokenize(text=text, language=\"english\")\n",
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {
    "papermill": {
     "duration": 0.004137,
     "end_time": "2025-09-14T07:41:06.208756",
     "exception": false,
     "start_time": "2025-09-14T07:41:06.204619",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### wordpunct_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {
    "papermill": {
     "duration": 0.012182,
     "end_time": "2025-09-14T07:41:06.225392",
     "exception": false,
     "start_time": "2025-09-14T07:41:06.213210",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "wordpunct = wordpunct_tokenize(text=text)\n",
    "wordpunct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {
    "papermill": {
     "duration": 0.004216,
     "end_time": "2025-09-14T07:41:06.234473",
     "exception": false,
     "start_time": "2025-09-14T07:41:06.230257",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {
    "papermill": {
     "duration": 0.011614,
     "end_time": "2025-09-14T07:41:06.250490",
     "exception": false,
     "start_time": "2025-09-14T07:41:06.238876",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "treebank = TreebankWordTokenizer().tokenize(text)\n",
    "treebank"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {
    "papermill": {
     "duration": 0.004223,
     "end_time": "2025-09-14T07:41:06.259180",
     "exception": false,
     "start_time": "2025-09-14T07:41:06.254957",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Stemming\n",
    "\n",
    "It is the process of reducing a word to its base form, known as the `stem`. This `stem` may **not** be a valid word in itself, but it serves as the foundation to which prefixes and suffixes are attached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {
    "papermill": {
     "duration": 0.010287,
     "end_time": "2025-09-14T07:41:06.273887",
     "exception": false,
     "start_time": "2025-09-14T07:41:06.263600",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer, SnowballStemmer, RegexpStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {
    "papermill": {
     "duration": 0.010573,
     "end_time": "2025-09-14T07:41:06.289094",
     "exception": false,
     "start_time": "2025-09-14T07:41:06.278521",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "words = [\n",
    "    \"fairly\",\n",
    "    \"goes\",\n",
    "    \"ingesting\",\n",
    "    \"eating\",\n",
    "    \"eats\",\n",
    "    \"eaten\",\n",
    "    \"writing\",\n",
    "    \"writes\",\n",
    "    \"programming\",\n",
    "    \"programs\",\n",
    "    \"history\",\n",
    "    \"finally\",\n",
    "    \"finalized\",\n",
    "    \"sportingly\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {
    "papermill": {
     "duration": 0.004595,
     "end_time": "2025-09-14T07:41:06.298233",
     "exception": false,
     "start_time": "2025-09-14T07:41:06.293638",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {
    "papermill": {
     "duration": 0.011755,
     "end_time": "2025-09-14T07:41:06.315423",
     "exception": false,
     "start_time": "2025-09-14T07:41:06.303668",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "porter = PorterStemmer()\n",
    "\n",
    "for word in words:\n",
    "    print(f\"{word:12} ----> {porter.stem(word)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {
    "papermill": {
     "duration": 0.004623,
     "end_time": "2025-09-14T07:41:06.324706",
     "exception": false,
     "start_time": "2025-09-14T07:41:06.320083",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {
    "papermill": {
     "duration": 0.011572,
     "end_time": "2025-09-14T07:41:06.340914",
     "exception": false,
     "start_time": "2025-09-14T07:41:06.329342",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "snowball = SnowballStemmer(language=\"english\", ignore_stopwords=False)\n",
    "\n",
    "for word in words:\n",
    "    print(f\"{word:12} ----> {snowball.stem(word)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {
    "papermill": {
     "duration": 0.004524,
     "end_time": "2025-09-14T07:41:06.350304",
     "exception": false,
     "start_time": "2025-09-14T07:41:06.345780",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### RegexpStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {
    "papermill": {
     "duration": 0.011689,
     "end_time": "2025-09-14T07:41:06.366584",
     "exception": false,
     "start_time": "2025-09-14T07:41:06.354895",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "regexp = RegexpStemmer(\"ing$|s$|e$|able$\", min=4)\n",
    "\n",
    "for word in words:\n",
    "    print(f\"{word:12} ----> {regexp.stem(word)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {
    "papermill": {
     "duration": 0.004549,
     "end_time": "2025-09-14T07:41:06.376035",
     "exception": false,
     "start_time": "2025-09-14T07:41:06.371486",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Lemmatization\n",
    "\n",
    "The process of reducing a word to its base or dictionary form, called a `lemma`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {
    "papermill": {
     "duration": 0.010768,
     "end_time": "2025-09-14T07:41:06.391600",
     "exception": false,
     "start_time": "2025-09-14T07:41:06.380832",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {
    "papermill": {
     "duration": 0.011014,
     "end_time": "2025-09-14T07:41:06.407425",
     "exception": false,
     "start_time": "2025-09-14T07:41:06.396411",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "words = [\n",
    "    \"fairly\",\n",
    "    \"goes\",\n",
    "    \"ingesting\",\n",
    "    \"eating\",\n",
    "    \"eats\",\n",
    "    \"eaten\",\n",
    "    \"writing\",\n",
    "    \"writes\",\n",
    "    \"programming\",\n",
    "    \"programs\",\n",
    "    \"history\",\n",
    "    \"finally\",\n",
    "    \"finalized\",\n",
    "    \"sportingly\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {
    "papermill": {
     "duration": 0.004641,
     "end_time": "2025-09-14T07:41:06.416921",
     "exception": false,
     "start_time": "2025-09-14T07:41:06.412280",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {
    "papermill": {
     "duration": 3.057692,
     "end_time": "2025-09-14T07:41:09.479418",
     "exception": false,
     "start_time": "2025-09-14T07:41:06.421726",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "nltk.download(\"wordnet\")\n",
    "\n",
    "wordnet = WordNetLemmatizer()\n",
    "\n",
    "for word in words:\n",
    "    print(f\"{word:12} ----> {wordnet.lemmatize(word=word, pos='v')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {
    "papermill": {
     "duration": 0.005178,
     "end_time": "2025-09-14T07:41:09.490130",
     "exception": false,
     "start_time": "2025-09-14T07:41:09.484952",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Stop Words\n",
    "\n",
    "Stop words are commonly occurring words in a language (like \"the\", \"a\", \"is\", \"in\", \"on\", \"and\") that often carry little to no semantic value for many Natural Language Processing (NLP) tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {
    "papermill": {
     "duration": 0.010808,
     "end_time": "2025-09-14T07:41:09.505889",
     "exception": false,
     "start_time": "2025-09-14T07:41:09.495081",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {
    "papermill": {
     "duration": 0.011362,
     "end_time": "2025-09-14T07:41:09.522236",
     "exception": false,
     "start_time": "2025-09-14T07:41:09.510874",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "paragraph = \"\"\"I have three visions for India. In 3000 years of our history, people from all over\n",
    "               the world have come and invaded us, captured our lands, conquered our minds.\n",
    "               From Alexander onwards, the Greeks, the Turks, the Moguls, the Portuguese, the British,\n",
    "               the French, the Dutch, all of them came and looted us, took over what was ours.\n",
    "               Yet we have not done this to any other nation. We have not conquered anyone.\n",
    "               We have not grabbed their land, their culture,\n",
    "               their history and tried to enforce our way of life on them.\n",
    "               Why? Because we respect the freedom of others.That is why my\n",
    "               first vision is that of freedom. I believe that India got its first vision of\n",
    "               this in 1857, when we started the War of Independence. It is this freedom that\n",
    "               we must protect and nurture and build on. If we are not free, no one will respect us.\n",
    "               My second vision for India’s development. For fifty years we have been a developing nation.\n",
    "               It is time we see ourselves as a developed nation. We are among the top 5 nations of the world\n",
    "               in terms of GDP. We have a 10 percent growth rate in most areas. Our poverty levels are falling.\n",
    "               Our achievements are being globally recognized today. Yet we lack the self-confidence to\n",
    "               see ourselves as a developed nation, self-reliant and self-assured. Isn’t this incorrect?\n",
    "               I have a third vision. India must stand up to the world. Because I believe that unless India\n",
    "               stands up to the world, no one will respect us. Only strength respects strength. We must be\n",
    "               strong not only as a military power but also as an economic power. Both must go hand-in-hand.\n",
    "               My good fortune was to have worked with three great minds. Dr. Vikram Sarabhai of the Dept. of\n",
    "               space, Professor Satish Dhawan, who succeeded him and Dr. Brahm Prakash, father of nuclear material.\n",
    "               I was lucky to have worked with all three of them closely and consider this the great opportunity of my life.\n",
    "               I see four milestones in my career\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {
    "papermill": {
     "duration": 0.021472,
     "end_time": "2025-09-14T07:41:09.548779",
     "exception": false,
     "start_time": "2025-09-14T07:41:09.527307",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "\n",
    "stop_words = stopwords.words(\"english\")\n",
    "\n",
    "# Remove stopwords from paragraph using `tokenizer`\n",
    "paragraph_tokens = sent_tokenize(paragraph)\n",
    "\n",
    "without_stop_word_sentences = []\n",
    "for sentence in paragraph_tokens:\n",
    "    word_tokens = word_tokenize(sentence)\n",
    "    words = [word for word in word_tokens if word not in stop_words]\n",
    "    sent = \" \".join(words)\n",
    "    without_stop_word_sentences.append(sent)\n",
    "\n",
    "print(without_stop_word_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {
    "papermill": {
     "duration": 0.018027,
     "end_time": "2025-09-14T07:41:09.572229",
     "exception": false,
     "start_time": "2025-09-14T07:41:09.554202",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Remove stopwords from paragraph using `lemmitization`\n",
    "paragraph_tokens = sent_tokenize(paragraph)\n",
    "\n",
    "# FIXME\n",
    "without_stop_word_sentences = []\n",
    "for sentence in paragraph_tokens:\n",
    "    lemma_sent = wordnet.lemmatize(sentence)\n",
    "    word_tokens = word_tokenize(lemma_sent)\n",
    "    words = [word for word in word_tokens if word not in stop_words]\n",
    "    sent = \" \".join(words)\n",
    "    without_stop_word_sentences.append(sent)\n",
    "\n",
    "print(without_stop_word_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {
    "papermill": {
     "duration": 0.005218,
     "end_time": "2025-09-14T07:41:09.583243",
     "exception": false,
     "start_time": "2025-09-14T07:41:09.578025",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 11.380313,
   "end_time": "2025-09-14T07:41:10.407248",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-09-14T07:40:59.026935",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
