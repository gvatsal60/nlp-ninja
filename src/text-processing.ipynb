{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee9eda2b",
   "metadata": {
    "papermill": {
     "duration": 0.005886,
     "end_time": "2025-09-12T08:53:52.061362",
     "exception": false,
     "start_time": "2025-09-12T08:53:52.055476",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# NLP - Pre-processing\n",
    "\n",
    "Corpus -> Paragraph\n",
    "\n",
    "Documents -> Sentences\n",
    "\n",
    "Vocabulary -> Unique Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99b50105",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-12T08:53:52.071798Z",
     "iopub.status.busy": "2025-09-12T08:53:52.071478Z",
     "iopub.status.idle": "2025-09-12T08:53:54.674340Z",
     "shell.execute_reply": "2025-09-12T08:53:54.673226Z"
    },
    "papermill": {
     "duration": 2.610371,
     "end_time": "2025-09-12T08:53:54.676344",
     "exception": false,
     "start_time": "2025-09-12T08:53:52.065973",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ceecec0",
   "metadata": {
    "papermill": {
     "duration": 0.004183,
     "end_time": "2025-09-12T08:53:54.684998",
     "exception": false,
     "start_time": "2025-09-12T08:53:54.680815",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Tokenization\n",
    "\n",
    "It is the fundamental process of breaking down a text into smaller, manageable units called `tokens`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87f62f74",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-12T08:53:54.694998Z",
     "iopub.status.busy": "2025-09-12T08:53:54.694507Z",
     "iopub.status.idle": "2025-09-12T08:53:54.699472Z",
     "shell.execute_reply": "2025-09-12T08:53:54.698451Z"
    },
    "papermill": {
     "duration": 0.011788,
     "end_time": "2025-09-12T08:53:54.701070",
     "exception": false,
     "start_time": "2025-09-12T08:53:54.689282",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import (\n",
    "    sent_tokenize,\n",
    "    word_tokenize,\n",
    "    wordpunct_tokenize,\n",
    "    TreebankWordTokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4d00168",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-12T08:53:54.711972Z",
     "iopub.status.busy": "2025-09-12T08:53:54.711255Z",
     "iopub.status.idle": "2025-09-12T08:53:54.715813Z",
     "shell.execute_reply": "2025-09-12T08:53:54.714901Z"
    },
    "papermill": {
     "duration": 0.011604,
     "end_time": "2025-09-12T08:53:54.717482",
     "exception": false,
     "start_time": "2025-09-12T08:53:54.705878",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "text = \"\"\"There are multiple ways we can perform cost's tokenization on given text data.\n",
    "We can choose any method based on language, library and purpose of modeling.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de99771",
   "metadata": {
    "papermill": {
     "duration": 0.005086,
     "end_time": "2025-09-12T08:53:54.726970",
     "exception": false,
     "start_time": "2025-09-12T08:53:54.721884",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "901f780a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-12T08:53:54.737028Z",
     "iopub.status.busy": "2025-09-12T08:53:54.736682Z",
     "iopub.status.idle": "2025-09-12T08:53:54.779334Z",
     "shell.execute_reply": "2025-09-12T08:53:54.778406Z"
    },
    "papermill": {
     "duration": 0.049974,
     "end_time": "2025-09-12T08:53:54.781107",
     "exception": false,
     "start_time": "2025-09-12T08:53:54.731133",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"There are multiple ways we can perform cost's tokenization on given text data.\",\n",
       " 'We can choose any method based on language, library and purpose of modeling.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = sent_tokenize(text=text, language='english')\n",
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ccc9324",
   "metadata": {
    "papermill": {
     "duration": 0.004255,
     "end_time": "2025-09-12T08:53:54.790183",
     "exception": false,
     "start_time": "2025-09-12T08:53:54.785928",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "845c3c24",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-12T08:53:54.800813Z",
     "iopub.status.busy": "2025-09-12T08:53:54.800494Z",
     "iopub.status.idle": "2025-09-12T08:53:54.807802Z",
     "shell.execute_reply": "2025-09-12T08:53:54.806728Z"
    },
    "papermill": {
     "duration": 0.014974,
     "end_time": "2025-09-12T08:53:54.809611",
     "exception": false,
     "start_time": "2025-09-12T08:53:54.794637",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['There',\n",
       " 'are',\n",
       " 'multiple',\n",
       " 'ways',\n",
       " 'we',\n",
       " 'can',\n",
       " 'perform',\n",
       " 'cost',\n",
       " \"'s\",\n",
       " 'tokenization',\n",
       " 'on',\n",
       " 'given',\n",
       " 'text',\n",
       " 'data',\n",
       " '.',\n",
       " 'We',\n",
       " 'can',\n",
       " 'choose',\n",
       " 'any',\n",
       " 'method',\n",
       " 'based',\n",
       " 'on',\n",
       " 'language',\n",
       " ',',\n",
       " 'library',\n",
       " 'and',\n",
       " 'purpose',\n",
       " 'of',\n",
       " 'modeling',\n",
       " '.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = word_tokenize(text=text, language='english')\n",
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d57963d",
   "metadata": {
    "papermill": {
     "duration": 0.004315,
     "end_time": "2025-09-12T08:53:54.818763",
     "exception": false,
     "start_time": "2025-09-12T08:53:54.814448",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### wordpunct_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10066be9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-12T08:53:54.829720Z",
     "iopub.status.busy": "2025-09-12T08:53:54.829370Z",
     "iopub.status.idle": "2025-09-12T08:53:55.023161Z",
     "shell.execute_reply": "2025-09-12T08:53:55.022118Z"
    },
    "papermill": {
     "duration": 0.201568,
     "end_time": "2025-09-12T08:53:55.024897",
     "exception": false,
     "start_time": "2025-09-12T08:53:54.823329",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['There',\n",
       " 'are',\n",
       " 'multiple',\n",
       " 'ways',\n",
       " 'we',\n",
       " 'can',\n",
       " 'perform',\n",
       " 'cost',\n",
       " \"'\",\n",
       " 's',\n",
       " 'tokenization',\n",
       " 'on',\n",
       " 'given',\n",
       " 'text',\n",
       " 'data',\n",
       " '.',\n",
       " 'We',\n",
       " 'can',\n",
       " 'choose',\n",
       " 'any',\n",
       " 'method',\n",
       " 'based',\n",
       " 'on',\n",
       " 'language',\n",
       " ',',\n",
       " 'library',\n",
       " 'and',\n",
       " 'purpose',\n",
       " 'of',\n",
       " 'modeling',\n",
       " '.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "\n",
    "wordpunct = wordpunct_tokenize(text=text)\n",
    "wordpunct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee371fe",
   "metadata": {
    "papermill": {
     "duration": 0.004808,
     "end_time": "2025-09-12T08:53:55.035189",
     "exception": false,
     "start_time": "2025-09-12T08:53:55.030381",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3e75e07",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-12T08:53:55.046688Z",
     "iopub.status.busy": "2025-09-12T08:53:55.046386Z",
     "iopub.status.idle": "2025-09-12T08:53:55.053347Z",
     "shell.execute_reply": "2025-09-12T08:53:55.052236Z"
    },
    "papermill": {
     "duration": 0.014385,
     "end_time": "2025-09-12T08:53:55.054934",
     "exception": false,
     "start_time": "2025-09-12T08:53:55.040549",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['There',\n",
       " 'are',\n",
       " 'multiple',\n",
       " 'ways',\n",
       " 'we',\n",
       " 'can',\n",
       " 'perform',\n",
       " 'cost',\n",
       " \"'s\",\n",
       " 'tokenization',\n",
       " 'on',\n",
       " 'given',\n",
       " 'text',\n",
       " 'data.',\n",
       " 'We',\n",
       " 'can',\n",
       " 'choose',\n",
       " 'any',\n",
       " 'method',\n",
       " 'based',\n",
       " 'on',\n",
       " 'language',\n",
       " ',',\n",
       " 'library',\n",
       " 'and',\n",
       " 'purpose',\n",
       " 'of',\n",
       " 'modeling',\n",
       " '.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "treebank = TreebankWordTokenizer().tokenize(text)\n",
    "treebank"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ed45ca",
   "metadata": {
    "papermill": {
     "duration": 0.005468,
     "end_time": "2025-09-12T08:53:55.065641",
     "exception": false,
     "start_time": "2025-09-12T08:53:55.060173",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Stemming\n",
    "\n",
    "It is the process of reducing a word to its base form, known as the `stem`. This `stem` may **not** be a valid word in itself, but it serves as the foundation to which prefixes and suffixes are attached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "454dd1bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-12T08:53:55.076757Z",
     "iopub.status.busy": "2025-09-12T08:53:55.076467Z",
     "iopub.status.idle": "2025-09-12T08:53:55.081465Z",
     "shell.execute_reply": "2025-09-12T08:53:55.080321Z"
    },
    "papermill": {
     "duration": 0.012657,
     "end_time": "2025-09-12T08:53:55.083344",
     "exception": false,
     "start_time": "2025-09-12T08:53:55.070687",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.stem import (\n",
    "    PorterStemmer,\n",
    "    SnowballStemmer,\n",
    "    RegexpStemmer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa1328fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-12T08:53:55.094634Z",
     "iopub.status.busy": "2025-09-12T08:53:55.094312Z",
     "iopub.status.idle": "2025-09-12T08:53:55.099496Z",
     "shell.execute_reply": "2025-09-12T08:53:55.098118Z"
    },
    "papermill": {
     "duration": 0.012795,
     "end_time": "2025-09-12T08:53:55.101284",
     "exception": false,
     "start_time": "2025-09-12T08:53:55.088489",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "words = [\"fairly\", \"goes\", \"ingesting\", \"eating\", \"eats\", \"eaten\", \"writing\",\n",
    "         \"writes\", \"programming\", \"programs\", \"history\", \"finally\", \"finalized\", \"sportingly\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716de9bb",
   "metadata": {
    "papermill": {
     "duration": 0.004811,
     "end_time": "2025-09-12T08:53:55.111197",
     "exception": false,
     "start_time": "2025-09-12T08:53:55.106386",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00b7c16c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-12T08:53:55.122108Z",
     "iopub.status.busy": "2025-09-12T08:53:55.121754Z",
     "iopub.status.idle": "2025-09-12T08:53:55.127719Z",
     "shell.execute_reply": "2025-09-12T08:53:55.126706Z"
    },
    "papermill": {
     "duration": 0.013155,
     "end_time": "2025-09-12T08:53:55.129187",
     "exception": false,
     "start_time": "2025-09-12T08:53:55.116032",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fairly       ----> fairli\n",
      "goes         ----> goe\n",
      "ingesting    ----> ingest\n",
      "eating       ----> eat\n",
      "eats         ----> eat\n",
      "eaten        ----> eaten\n",
      "writing      ----> write\n",
      "writes       ----> write\n",
      "programming  ----> program\n",
      "programs     ----> program\n",
      "history      ----> histori\n",
      "finally      ----> final\n",
      "finalized    ----> final\n",
      "sportingly   ----> sportingli\n"
     ]
    }
   ],
   "source": [
    "porter = PorterStemmer()\n",
    "\n",
    "for word in words:\n",
    "    print(f\"{word:12} ----> {porter.stem(word)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7449513f",
   "metadata": {
    "papermill": {
     "duration": 0.004691,
     "end_time": "2025-09-12T08:53:55.139047",
     "exception": false,
     "start_time": "2025-09-12T08:53:55.134356",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1dbbdd79",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-12T08:53:55.150686Z",
     "iopub.status.busy": "2025-09-12T08:53:55.150381Z",
     "iopub.status.idle": "2025-09-12T08:53:55.155628Z",
     "shell.execute_reply": "2025-09-12T08:53:55.154816Z"
    },
    "papermill": {
     "duration": 0.013289,
     "end_time": "2025-09-12T08:53:55.157357",
     "exception": false,
     "start_time": "2025-09-12T08:53:55.144068",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fairly       ----> fair\n",
      "goes         ----> goe\n",
      "ingesting    ----> ingest\n",
      "eating       ----> eat\n",
      "eats         ----> eat\n",
      "eaten        ----> eaten\n",
      "writing      ----> write\n",
      "writes       ----> write\n",
      "programming  ----> program\n",
      "programs     ----> program\n",
      "history      ----> histori\n",
      "finally      ----> final\n",
      "finalized    ----> final\n",
      "sportingly   ----> sport\n"
     ]
    }
   ],
   "source": [
    "snowball = SnowballStemmer(language='english', ignore_stopwords=False)\n",
    "\n",
    "for word in words:\n",
    "    print(f\"{word:12} ----> {snowball.stem(word)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1b60b1",
   "metadata": {
    "papermill": {
     "duration": 0.005261,
     "end_time": "2025-09-12T08:53:55.168022",
     "exception": false,
     "start_time": "2025-09-12T08:53:55.162761",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### RegexpStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9bedbbb0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-12T08:53:55.180705Z",
     "iopub.status.busy": "2025-09-12T08:53:55.180378Z",
     "iopub.status.idle": "2025-09-12T08:53:55.185744Z",
     "shell.execute_reply": "2025-09-12T08:53:55.184776Z"
    },
    "papermill": {
     "duration": 0.01326,
     "end_time": "2025-09-12T08:53:55.187253",
     "exception": false,
     "start_time": "2025-09-12T08:53:55.173993",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fairly       ----> fairly\n",
      "goes         ----> goe\n",
      "ingesting    ----> ingest\n",
      "eating       ----> eat\n",
      "eats         ----> eat\n",
      "eaten        ----> eaten\n",
      "writing      ----> writ\n",
      "writes       ----> write\n",
      "programming  ----> programm\n",
      "programs     ----> program\n",
      "history      ----> history\n",
      "finally      ----> finally\n",
      "finalized    ----> finalized\n",
      "sportingly   ----> sportingly\n"
     ]
    }
   ],
   "source": [
    "regexp = RegexpStemmer('ing$|s$|e$|able$', min=4)\n",
    "\n",
    "for word in words:\n",
    "    print(f\"{word:12} ----> {regexp.stem(word)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afa5ad5",
   "metadata": {
    "papermill": {
     "duration": 0.00533,
     "end_time": "2025-09-12T08:53:55.197890",
     "exception": false,
     "start_time": "2025-09-12T08:53:55.192560",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Lemmatization\n",
    "\n",
    "The process of reducing a word to its base or dictionary form, called a `lemma`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b0918414",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-12T08:53:55.210012Z",
     "iopub.status.busy": "2025-09-12T08:53:55.209667Z",
     "iopub.status.idle": "2025-09-12T08:53:55.213709Z",
     "shell.execute_reply": "2025-09-12T08:53:55.212865Z"
    },
    "papermill": {
     "duration": 0.012108,
     "end_time": "2025-09-12T08:53:55.215281",
     "exception": false,
     "start_time": "2025-09-12T08:53:55.203173",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.stem import (\n",
    "    WordNetLemmatizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "16edc048",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-12T08:53:55.228064Z",
     "iopub.status.busy": "2025-09-12T08:53:55.227704Z",
     "iopub.status.idle": "2025-09-12T08:53:55.232772Z",
     "shell.execute_reply": "2025-09-12T08:53:55.231767Z"
    },
    "papermill": {
     "duration": 0.013172,
     "end_time": "2025-09-12T08:53:55.234428",
     "exception": false,
     "start_time": "2025-09-12T08:53:55.221256",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "words = [\"fairly\", \"goes\", \"ingesting\", \"eating\", \"eats\", \"eaten\", \"writing\",\n",
    "         \"writes\", \"programming\", \"programs\", \"history\", \"finally\", \"finalized\", \"sportingly\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afa5a43",
   "metadata": {
    "papermill": {
     "duration": 0.005286,
     "end_time": "2025-09-12T08:53:55.245421",
     "exception": false,
     "start_time": "2025-09-12T08:53:55.240135",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "88f2eac5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-12T08:53:55.257082Z",
     "iopub.status.busy": "2025-09-12T08:53:55.256752Z",
     "iopub.status.idle": "2025-09-12T08:53:58.538063Z",
     "shell.execute_reply": "2025-09-12T08:53:58.536925Z"
    },
    "papermill": {
     "duration": 3.288938,
     "end_time": "2025-09-12T08:53:58.539588",
     "exception": false,
     "start_time": "2025-09-12T08:53:55.250650",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fairly       ----> fairly\n",
      "goes         ----> go\n",
      "ingesting    ----> ingest\n",
      "eating       ----> eat\n",
      "eats         ----> eat\n",
      "eaten        ----> eat\n",
      "writing      ----> write\n",
      "writes       ----> write\n",
      "programming  ----> program\n",
      "programs     ----> program\n",
      "history      ----> history\n",
      "finally      ----> finally\n",
      "finalized    ----> finalize\n",
      "sportingly   ----> sportingly\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "\n",
    "wordnet = WordNetLemmatizer()\n",
    "\n",
    "for word in words:\n",
    "    print(f\"{word:12} ----> {wordnet.lemmatize(word=word, pos='v')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8316b97e",
   "metadata": {
    "papermill": {
     "duration": 0.005288,
     "end_time": "2025-09-12T08:53:58.550618",
     "exception": false,
     "start_time": "2025-09-12T08:53:58.545330",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Stop Words\n",
    "\n",
    "Stop words are commonly occurring words in a language (like \"the\", \"a\", \"is\", \"in\", \"on\", \"and\") that often carry little to no semantic value for many Natural Language Processing (NLP) tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "66331537",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-12T08:53:58.562926Z",
     "iopub.status.busy": "2025-09-12T08:53:58.562615Z",
     "iopub.status.idle": "2025-09-12T08:53:58.566982Z",
     "shell.execute_reply": "2025-09-12T08:53:58.566092Z"
    },
    "papermill": {
     "duration": 0.012202,
     "end_time": "2025-09-12T08:53:58.568401",
     "exception": false,
     "start_time": "2025-09-12T08:53:58.556199",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import (\n",
    "stopwords\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d6503d51",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-12T08:53:58.581410Z",
     "iopub.status.busy": "2025-09-12T08:53:58.581111Z",
     "iopub.status.idle": "2025-09-12T08:53:58.586567Z",
     "shell.execute_reply": "2025-09-12T08:53:58.585642Z"
    },
    "papermill": {
     "duration": 0.013574,
     "end_time": "2025-09-12T08:53:58.587985",
     "exception": false,
     "start_time": "2025-09-12T08:53:58.574411",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "paragraph = \"\"\"I have three visions for India. In 3000 years of our history, people from all over \n",
    "               the world have come and invaded us, captured our lands, conquered our minds. \n",
    "               From Alexander onwards, the Greeks, the Turks, the Moguls, the Portuguese, the British,\n",
    "               the French, the Dutch, all of them came and looted us, took over what was ours. \n",
    "               Yet we have not done this to any other nation. We have not conquered anyone. \n",
    "               We have not grabbed their land, their culture, \n",
    "               their history and tried to enforce our way of life on them. \n",
    "               Why? Because we respect the freedom of others.That is why my \n",
    "               first vision is that of freedom. I believe that India got its first vision of \n",
    "               this in 1857, when we started the War of Independence. It is this freedom that\n",
    "               we must protect and nurture and build on. If we are not free, no one will respect us.\n",
    "               My second vision for India’s development. For fifty years we have been a developing nation.\n",
    "               It is time we see ourselves as a developed nation. We are among the top 5 nations of the world\n",
    "               in terms of GDP. We have a 10 percent growth rate in most areas. Our poverty levels are falling.\n",
    "               Our achievements are being globally recognised today. Yet we lack the self-confidence to\n",
    "               see ourselves as a developed nation, self-reliant and self-assured. Isn’t this incorrect?\n",
    "               I have a third vision. India must stand up to the world. Because I believe that unless India \n",
    "               stands up to the world, no one will respect us. Only strength respects strength. We must be \n",
    "               strong not only as a military power but also as an economic power. Both must go hand-in-hand. \n",
    "               My good fortune was to have worked with three great minds. Dr. Vikram Sarabhai of the Dept. of \n",
    "               space, Professor Satish Dhawan, who succeeded him and Dr. Brahm Prakash, father of nuclear material.\n",
    "               I was lucky to have worked with all three of them closely and consider this the great opportunity of my life. \n",
    "               I see four milestones in my career\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7bd29fa8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-12T08:53:58.600512Z",
     "iopub.status.busy": "2025-09-12T08:53:58.599943Z",
     "iopub.status.idle": "2025-09-12T08:53:58.610993Z",
     "shell.execute_reply": "2025-09-12T08:53:58.610008Z"
    },
    "papermill": {
     "duration": 0.019048,
     "end_time": "2025-09-12T08:53:58.612591",
     "exception": false,
     "start_time": "2025-09-12T08:53:58.593543",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "# Remove stopwords from paragraph\n",
    "paragraph_tokens = sent_tokenize(paragraph)\n",
    "\n",
    "# print(paragraph_tokens)\n",
    "\n",
    "# without_stop_word_sentences = []\n",
    "# for sentence in paragraph_tokens:\n",
    "#     word_tokens = word_tokenize(sentence)\n",
    "#     words = [word for word in word_tokens if word not in stopwords]\n",
    "#     without_stop_word_sentences.extend(' '.join(words))\n",
    "    \n",
    "# print(without_stop_word_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288a4318",
   "metadata": {
    "papermill": {
     "duration": 0.005445,
     "end_time": "2025-09-12T08:53:58.623774",
     "exception": false,
     "start_time": "2025-09-12T08:53:58.618329",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 13.499536,
   "end_time": "2025-09-12T08:53:59.650436",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-09-12T08:53:46.150900",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
