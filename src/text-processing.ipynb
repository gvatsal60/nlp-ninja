{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f870312",
   "metadata": {
    "papermill": {
     "duration": 0.004821,
     "end_time": "2025-09-14T07:41:03.916069",
     "exception": false,
     "start_time": "2025-09-14T07:41:03.911248",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# NLP - Pre-processing\n",
    "\n",
    "Corpus -> Paragraph\n",
    "\n",
    "Documents -> Sentences\n",
    "\n",
    "Vocabulary -> Unique Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97a0b1a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T07:41:03.925946Z",
     "iopub.status.busy": "2025-09-14T07:41:03.925139Z",
     "iopub.status.idle": "2025-09-14T07:41:05.970098Z",
     "shell.execute_reply": "2025-09-14T07:41:05.969337Z"
    },
    "papermill": {
     "duration": 2.051446,
     "end_time": "2025-09-14T07:41:05.971623",
     "exception": false,
     "start_time": "2025-09-14T07:41:03.920177",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b471463",
   "metadata": {
    "papermill": {
     "duration": 0.004106,
     "end_time": "2025-09-14T07:41:05.980047",
     "exception": false,
     "start_time": "2025-09-14T07:41:05.975941",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Tokenization\n",
    "\n",
    "It is the fundamental process of breaking down a text into smaller, manageable units called `tokens`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c97a708e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T07:41:05.989215Z",
     "iopub.status.busy": "2025-09-14T07:41:05.988834Z",
     "iopub.status.idle": "2025-09-14T07:41:05.992750Z",
     "shell.execute_reply": "2025-09-14T07:41:05.992121Z"
    },
    "papermill": {
     "duration": 0.009913,
     "end_time": "2025-09-14T07:41:05.993944",
     "exception": false,
     "start_time": "2025-09-14T07:41:05.984031",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import (\n",
    "    sent_tokenize,\n",
    "    word_tokenize,\n",
    "    wordpunct_tokenize,\n",
    "    TreebankWordTokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f4c3e2e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T07:41:06.003175Z",
     "iopub.status.busy": "2025-09-14T07:41:06.002888Z",
     "iopub.status.idle": "2025-09-14T07:41:06.006833Z",
     "shell.execute_reply": "2025-09-14T07:41:06.006194Z"
    },
    "papermill": {
     "duration": 0.010026,
     "end_time": "2025-09-14T07:41:06.008080",
     "exception": false,
     "start_time": "2025-09-14T07:41:05.998054",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "text = \"\"\"There are multiple ways we can perform cost's tokenization on given text data.\n",
    "We can choose any method based on language, library and purpose of modeling.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c28c87",
   "metadata": {
    "papermill": {
     "duration": 0.004637,
     "end_time": "2025-09-14T07:41:06.016838",
     "exception": false,
     "start_time": "2025-09-14T07:41:06.012201",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "daf7f7a5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T07:41:06.025551Z",
     "iopub.status.busy": "2025-09-14T07:41:06.025275Z",
     "iopub.status.idle": "2025-09-14T07:41:06.173175Z",
     "shell.execute_reply": "2025-09-14T07:41:06.172187Z"
    },
    "papermill": {
     "duration": 0.153874,
     "end_time": "2025-09-14T07:41:06.174571",
     "exception": false,
     "start_time": "2025-09-14T07:41:06.020697",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /usr/share/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"There are multiple ways we can perform cost's tokenization on given text data.\",\n",
       " 'We can choose any method based on language, library and purpose of modeling.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"punkt_tab\")\n",
    "\n",
    "sentences = sent_tokenize(text=text, language=\"english\")\n",
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0840c1b",
   "metadata": {
    "papermill": {
     "duration": 0.004025,
     "end_time": "2025-09-14T07:41:06.183019",
     "exception": false,
     "start_time": "2025-09-14T07:41:06.178994",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2eed0792",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T07:41:06.192509Z",
     "iopub.status.busy": "2025-09-14T07:41:06.192226Z",
     "iopub.status.idle": "2025-09-14T07:41:06.198389Z",
     "shell.execute_reply": "2025-09-14T07:41:06.197629Z"
    },
    "papermill": {
     "duration": 0.012407,
     "end_time": "2025-09-14T07:41:06.199700",
     "exception": false,
     "start_time": "2025-09-14T07:41:06.187293",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['There',\n",
       " 'are',\n",
       " 'multiple',\n",
       " 'ways',\n",
       " 'we',\n",
       " 'can',\n",
       " 'perform',\n",
       " 'cost',\n",
       " \"'s\",\n",
       " 'tokenization',\n",
       " 'on',\n",
       " 'given',\n",
       " 'text',\n",
       " 'data',\n",
       " '.',\n",
       " 'We',\n",
       " 'can',\n",
       " 'choose',\n",
       " 'any',\n",
       " 'method',\n",
       " 'based',\n",
       " 'on',\n",
       " 'language',\n",
       " ',',\n",
       " 'library',\n",
       " 'and',\n",
       " 'purpose',\n",
       " 'of',\n",
       " 'modeling',\n",
       " '.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = word_tokenize(text=text, language=\"english\")\n",
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e7c924",
   "metadata": {
    "papermill": {
     "duration": 0.004137,
     "end_time": "2025-09-14T07:41:06.208756",
     "exception": false,
     "start_time": "2025-09-14T07:41:06.204619",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### wordpunct_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2731fc13",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T07:41:06.218985Z",
     "iopub.status.busy": "2025-09-14T07:41:06.218292Z",
     "iopub.status.idle": "2025-09-14T07:41:06.223976Z",
     "shell.execute_reply": "2025-09-14T07:41:06.223305Z"
    },
    "papermill": {
     "duration": 0.012182,
     "end_time": "2025-09-14T07:41:06.225392",
     "exception": false,
     "start_time": "2025-09-14T07:41:06.213210",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['There',\n",
       " 'are',\n",
       " 'multiple',\n",
       " 'ways',\n",
       " 'we',\n",
       " 'can',\n",
       " 'perform',\n",
       " 'cost',\n",
       " \"'\",\n",
       " 's',\n",
       " 'tokenization',\n",
       " 'on',\n",
       " 'given',\n",
       " 'text',\n",
       " 'data',\n",
       " '.',\n",
       " 'We',\n",
       " 'can',\n",
       " 'choose',\n",
       " 'any',\n",
       " 'method',\n",
       " 'based',\n",
       " 'on',\n",
       " 'language',\n",
       " ',',\n",
       " 'library',\n",
       " 'and',\n",
       " 'purpose',\n",
       " 'of',\n",
       " 'modeling',\n",
       " '.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordpunct = wordpunct_tokenize(text=text)\n",
    "wordpunct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ffbf58f",
   "metadata": {
    "papermill": {
     "duration": 0.004216,
     "end_time": "2025-09-14T07:41:06.234473",
     "exception": false,
     "start_time": "2025-09-14T07:41:06.230257",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d765b5b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T07:41:06.244397Z",
     "iopub.status.busy": "2025-09-14T07:41:06.244145Z",
     "iopub.status.idle": "2025-09-14T07:41:06.249409Z",
     "shell.execute_reply": "2025-09-14T07:41:06.248795Z"
    },
    "papermill": {
     "duration": 0.011614,
     "end_time": "2025-09-14T07:41:06.250490",
     "exception": false,
     "start_time": "2025-09-14T07:41:06.238876",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['There',\n",
       " 'are',\n",
       " 'multiple',\n",
       " 'ways',\n",
       " 'we',\n",
       " 'can',\n",
       " 'perform',\n",
       " 'cost',\n",
       " \"'s\",\n",
       " 'tokenization',\n",
       " 'on',\n",
       " 'given',\n",
       " 'text',\n",
       " 'data.',\n",
       " 'We',\n",
       " 'can',\n",
       " 'choose',\n",
       " 'any',\n",
       " 'method',\n",
       " 'based',\n",
       " 'on',\n",
       " 'language',\n",
       " ',',\n",
       " 'library',\n",
       " 'and',\n",
       " 'purpose',\n",
       " 'of',\n",
       " 'modeling',\n",
       " '.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "treebank = TreebankWordTokenizer().tokenize(text)\n",
    "treebank"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9ce9e1",
   "metadata": {
    "papermill": {
     "duration": 0.004223,
     "end_time": "2025-09-14T07:41:06.259180",
     "exception": false,
     "start_time": "2025-09-14T07:41:06.254957",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Stemming\n",
    "\n",
    "It is the process of reducing a word to its base form, known as the `stem`. This `stem` may **not** be a valid word in itself, but it serves as the foundation to which prefixes and suffixes are attached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de94cf9f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T07:41:06.269482Z",
     "iopub.status.busy": "2025-09-14T07:41:06.268794Z",
     "iopub.status.idle": "2025-09-14T07:41:06.272558Z",
     "shell.execute_reply": "2025-09-14T07:41:06.271934Z"
    },
    "papermill": {
     "duration": 0.010287,
     "end_time": "2025-09-14T07:41:06.273887",
     "exception": false,
     "start_time": "2025-09-14T07:41:06.263600",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer, SnowballStemmer, RegexpStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc499163",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T07:41:06.284037Z",
     "iopub.status.busy": "2025-09-14T07:41:06.283801Z",
     "iopub.status.idle": "2025-09-14T07:41:06.287843Z",
     "shell.execute_reply": "2025-09-14T07:41:06.287114Z"
    },
    "papermill": {
     "duration": 0.010573,
     "end_time": "2025-09-14T07:41:06.289094",
     "exception": false,
     "start_time": "2025-09-14T07:41:06.278521",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "words = [\n",
    "    \"fairly\",\n",
    "    \"goes\",\n",
    "    \"ingesting\",\n",
    "    \"eating\",\n",
    "    \"eats\",\n",
    "    \"eaten\",\n",
    "    \"writing\",\n",
    "    \"writes\",\n",
    "    \"programming\",\n",
    "    \"programs\",\n",
    "    \"history\",\n",
    "    \"finally\",\n",
    "    \"finalized\",\n",
    "    \"sportingly\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c3d9bf",
   "metadata": {
    "papermill": {
     "duration": 0.004595,
     "end_time": "2025-09-14T07:41:06.298233",
     "exception": false,
     "start_time": "2025-09-14T07:41:06.293638",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3979c79",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T07:41:06.309258Z",
     "iopub.status.busy": "2025-09-14T07:41:06.309001Z",
     "iopub.status.idle": "2025-09-14T07:41:06.314126Z",
     "shell.execute_reply": "2025-09-14T07:41:06.313200Z"
    },
    "papermill": {
     "duration": 0.011755,
     "end_time": "2025-09-14T07:41:06.315423",
     "exception": false,
     "start_time": "2025-09-14T07:41:06.303668",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fairly       ----> fairli\n",
      "goes         ----> goe\n",
      "ingesting    ----> ingest\n",
      "eating       ----> eat\n",
      "eats         ----> eat\n",
      "eaten        ----> eaten\n",
      "writing      ----> write\n",
      "writes       ----> write\n",
      "programming  ----> program\n",
      "programs     ----> program\n",
      "history      ----> histori\n",
      "finally      ----> final\n",
      "finalized    ----> final\n",
      "sportingly   ----> sportingli\n"
     ]
    }
   ],
   "source": [
    "porter = PorterStemmer()\n",
    "\n",
    "for word in words:\n",
    "    print(f\"{word:12} ----> {porter.stem(word)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36130bb",
   "metadata": {
    "papermill": {
     "duration": 0.004623,
     "end_time": "2025-09-14T07:41:06.324706",
     "exception": false,
     "start_time": "2025-09-14T07:41:06.320083",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9ba97018",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T07:41:06.335024Z",
     "iopub.status.busy": "2025-09-14T07:41:06.334750Z",
     "iopub.status.idle": "2025-09-14T07:41:06.339567Z",
     "shell.execute_reply": "2025-09-14T07:41:06.338591Z"
    },
    "papermill": {
     "duration": 0.011572,
     "end_time": "2025-09-14T07:41:06.340914",
     "exception": false,
     "start_time": "2025-09-14T07:41:06.329342",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fairly       ----> fair\n",
      "goes         ----> goe\n",
      "ingesting    ----> ingest\n",
      "eating       ----> eat\n",
      "eats         ----> eat\n",
      "eaten        ----> eaten\n",
      "writing      ----> write\n",
      "writes       ----> write\n",
      "programming  ----> program\n",
      "programs     ----> program\n",
      "history      ----> histori\n",
      "finally      ----> final\n",
      "finalized    ----> final\n",
      "sportingly   ----> sport\n"
     ]
    }
   ],
   "source": [
    "snowball = SnowballStemmer(language=\"english\", ignore_stopwords=False)\n",
    "\n",
    "for word in words:\n",
    "    print(f\"{word:12} ----> {snowball.stem(word)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ae5dc3",
   "metadata": {
    "papermill": {
     "duration": 0.004524,
     "end_time": "2025-09-14T07:41:06.350304",
     "exception": false,
     "start_time": "2025-09-14T07:41:06.345780",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### RegexpStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "383fd16c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T07:41:06.360635Z",
     "iopub.status.busy": "2025-09-14T07:41:06.360376Z",
     "iopub.status.idle": "2025-09-14T07:41:06.365355Z",
     "shell.execute_reply": "2025-09-14T07:41:06.364534Z"
    },
    "papermill": {
     "duration": 0.011689,
     "end_time": "2025-09-14T07:41:06.366584",
     "exception": false,
     "start_time": "2025-09-14T07:41:06.354895",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fairly       ----> fairly\n",
      "goes         ----> goe\n",
      "ingesting    ----> ingest\n",
      "eating       ----> eat\n",
      "eats         ----> eat\n",
      "eaten        ----> eaten\n",
      "writing      ----> writ\n",
      "writes       ----> write\n",
      "programming  ----> programm\n",
      "programs     ----> program\n",
      "history      ----> history\n",
      "finally      ----> finally\n",
      "finalized    ----> finalized\n",
      "sportingly   ----> sportingly\n"
     ]
    }
   ],
   "source": [
    "regexp = RegexpStemmer(\"ing$|s$|e$|able$\", min=4)\n",
    "\n",
    "for word in words:\n",
    "    print(f\"{word:12} ----> {regexp.stem(word)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d48976",
   "metadata": {
    "papermill": {
     "duration": 0.004549,
     "end_time": "2025-09-14T07:41:06.376035",
     "exception": false,
     "start_time": "2025-09-14T07:41:06.371486",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Lemmatization\n",
    "\n",
    "The process of reducing a word to its base or dictionary form, called a `lemma`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8fe7e6a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T07:41:06.387309Z",
     "iopub.status.busy": "2025-09-14T07:41:06.386466Z",
     "iopub.status.idle": "2025-09-14T07:41:06.390412Z",
     "shell.execute_reply": "2025-09-14T07:41:06.389779Z"
    },
    "papermill": {
     "duration": 0.010768,
     "end_time": "2025-09-14T07:41:06.391600",
     "exception": false,
     "start_time": "2025-09-14T07:41:06.380832",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b483e9cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T07:41:06.402537Z",
     "iopub.status.busy": "2025-09-14T07:41:06.401892Z",
     "iopub.status.idle": "2025-09-14T07:41:06.406316Z",
     "shell.execute_reply": "2025-09-14T07:41:06.405473Z"
    },
    "papermill": {
     "duration": 0.011014,
     "end_time": "2025-09-14T07:41:06.407425",
     "exception": false,
     "start_time": "2025-09-14T07:41:06.396411",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "words = [\n",
    "    \"fairly\",\n",
    "    \"goes\",\n",
    "    \"ingesting\",\n",
    "    \"eating\",\n",
    "    \"eats\",\n",
    "    \"eaten\",\n",
    "    \"writing\",\n",
    "    \"writes\",\n",
    "    \"programming\",\n",
    "    \"programs\",\n",
    "    \"history\",\n",
    "    \"finally\",\n",
    "    \"finalized\",\n",
    "    \"sportingly\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac51eb8",
   "metadata": {
    "papermill": {
     "duration": 0.004641,
     "end_time": "2025-09-14T07:41:06.416921",
     "exception": false,
     "start_time": "2025-09-14T07:41:06.412280",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9d8d7f4f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T07:41:06.427827Z",
     "iopub.status.busy": "2025-09-14T07:41:06.427281Z",
     "iopub.status.idle": "2025-09-14T07:41:09.478030Z",
     "shell.execute_reply": "2025-09-14T07:41:09.477032Z"
    },
    "papermill": {
     "duration": 3.057692,
     "end_time": "2025-09-14T07:41:09.479418",
     "exception": false,
     "start_time": "2025-09-14T07:41:06.421726",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fairly       ----> fairly\n",
      "goes         ----> go\n",
      "ingesting    ----> ingest\n",
      "eating       ----> eat\n",
      "eats         ----> eat\n",
      "eaten        ----> eat\n",
      "writing      ----> write\n",
      "writes       ----> write\n",
      "programming  ----> program\n",
      "programs     ----> program\n",
      "history      ----> history\n",
      "finally      ----> finally\n",
      "finalized    ----> finalize\n",
      "sportingly   ----> sportingly\n"
     ]
    }
   ],
   "source": [
    "nltk.download(\"wordnet\")\n",
    "\n",
    "wordnet = WordNetLemmatizer()\n",
    "\n",
    "for word in words:\n",
    "    print(f\"{word:12} ----> {wordnet.lemmatize(word=word, pos='v')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130412d3",
   "metadata": {
    "papermill": {
     "duration": 0.005178,
     "end_time": "2025-09-14T07:41:09.490130",
     "exception": false,
     "start_time": "2025-09-14T07:41:09.484952",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Stop Words\n",
    "\n",
    "Stop words are commonly occurring words in a language (like \"the\", \"a\", \"is\", \"in\", \"on\", \"and\") that often carry little to no semantic value for many Natural Language Processing (NLP) tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e6ccf30b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T07:41:09.501284Z",
     "iopub.status.busy": "2025-09-14T07:41:09.500724Z",
     "iopub.status.idle": "2025-09-14T07:41:09.504721Z",
     "shell.execute_reply": "2025-09-14T07:41:09.503960Z"
    },
    "papermill": {
     "duration": 0.010808,
     "end_time": "2025-09-14T07:41:09.505889",
     "exception": false,
     "start_time": "2025-09-14T07:41:09.495081",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4f08a3d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T07:41:09.516911Z",
     "iopub.status.busy": "2025-09-14T07:41:09.516667Z",
     "iopub.status.idle": "2025-09-14T07:41:09.521032Z",
     "shell.execute_reply": "2025-09-14T07:41:09.520298Z"
    },
    "papermill": {
     "duration": 0.011362,
     "end_time": "2025-09-14T07:41:09.522236",
     "exception": false,
     "start_time": "2025-09-14T07:41:09.510874",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "paragraph = \"\"\"I have three visions for India. In 3000 years of our history, people from all over\n",
    "               the world have come and invaded us, captured our lands, conquered our minds.\n",
    "               From Alexander onwards, the Greeks, the Turks, the Moguls, the Portuguese, the British,\n",
    "               the French, the Dutch, all of them came and looted us, took over what was ours.\n",
    "               Yet we have not done this to any other nation. We have not conquered anyone.\n",
    "               We have not grabbed their land, their culture,\n",
    "               their history and tried to enforce our way of life on them.\n",
    "               Why? Because we respect the freedom of others.That is why my\n",
    "               first vision is that of freedom. I believe that India got its first vision of\n",
    "               this in 1857, when we started the War of Independence. It is this freedom that\n",
    "               we must protect and nurture and build on. If we are not free, no one will respect us.\n",
    "               My second vision for India’s development. For fifty years we have been a developing nation.\n",
    "               It is time we see ourselves as a developed nation. We are among the top 5 nations of the world\n",
    "               in terms of GDP. We have a 10 percent growth rate in most areas. Our poverty levels are falling.\n",
    "               Our achievements are being globally recognized today. Yet we lack the self-confidence to\n",
    "               see ourselves as a developed nation, self-reliant and self-assured. Isn’t this incorrect?\n",
    "               I have a third vision. India must stand up to the world. Because I believe that unless India\n",
    "               stands up to the world, no one will respect us. Only strength respects strength. We must be\n",
    "               strong not only as a military power but also as an economic power. Both must go hand-in-hand.\n",
    "               My good fortune was to have worked with three great minds. Dr. Vikram Sarabhai of the Dept. of\n",
    "               space, Professor Satish Dhawan, who succeeded him and Dr. Brahm Prakash, father of nuclear material.\n",
    "               I was lucky to have worked with all three of them closely and consider this the great opportunity of my life.\n",
    "               I see four milestones in my career\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a6ee9b4a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T07:41:09.533163Z",
     "iopub.status.busy": "2025-09-14T07:41:09.532931Z",
     "iopub.status.idle": "2025-09-14T07:41:09.547452Z",
     "shell.execute_reply": "2025-09-14T07:41:09.546505Z"
    },
    "papermill": {
     "duration": 0.021472,
     "end_time": "2025-09-14T07:41:09.548779",
     "exception": false,
     "start_time": "2025-09-14T07:41:09.527307",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I three visions India .', 'In 3000 years history , people world come invaded us , captured lands , conquered minds .', 'From Alexander onwards , Greeks , Turks , Moguls , Portuguese , British , French , Dutch , came looted us , took .', 'Yet done nation .', 'We conquered anyone .', 'We grabbed land , culture , history tried enforce way life .', 'Why ?', 'Because respect freedom others.That first vision freedom .', 'I believe India got first vision 1857 , started War Independence .', 'It freedom must protect nurture build .', 'If free , one respect us .', 'My second vision India ’ development .', 'For fifty years developing nation .', 'It time see developed nation .', 'We among top 5 nations world terms GDP .', 'We 10 percent growth rate areas .', 'Our poverty levels falling .', 'Our achievements globally recognized today .', 'Yet lack self-confidence see developed nation , self-reliant self-assured .', 'Isn ’ incorrect ?', 'I third vision .', 'India must stand world .', 'Because I believe unless India stands world , one respect us .', 'Only strength respects strength .', 'We must strong military power also economic power .', 'Both must go hand-in-hand .', 'My good fortune worked three great minds .', 'Dr. Vikram Sarabhai Dept .', 'space , Professor Satish Dhawan , succeeded Dr. Brahm Prakash , father nuclear material .', 'I lucky worked three closely consider great opportunity life .', 'I see four milestones career']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "\n",
    "stop_words = stopwords.words(\"english\")\n",
    "\n",
    "# Remove stopwords from paragraph using `tokenizer`\n",
    "paragraph_tokens = sent_tokenize(paragraph)\n",
    "\n",
    "without_stop_word_sentences = []\n",
    "for sentence in paragraph_tokens:\n",
    "    word_tokens = word_tokenize(sentence)\n",
    "    words = [word for word in word_tokens if word not in stop_words]\n",
    "    sent = \" \".join(words)\n",
    "    without_stop_word_sentences.append(sent)\n",
    "\n",
    "print(without_stop_word_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7771dbff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T07:41:09.560519Z",
     "iopub.status.busy": "2025-09-14T07:41:09.560255Z",
     "iopub.status.idle": "2025-09-14T07:41:09.570758Z",
     "shell.execute_reply": "2025-09-14T07:41:09.569930Z"
    },
    "papermill": {
     "duration": 0.018027,
     "end_time": "2025-09-14T07:41:09.572229",
     "exception": false,
     "start_time": "2025-09-14T07:41:09.554202",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I three visions India .', 'In 3000 years history , people world come invaded us , captured lands , conquered minds .', 'From Alexander onwards , Greeks , Turks , Moguls , Portuguese , British , French , Dutch , came looted us , took .', 'Yet done nation .', 'We conquered anyone .', 'We grabbed land , culture , history tried enforce way life .', 'Why ?', 'Because respect freedom others.That first vision freedom .', 'I believe India got first vision 1857 , started War Independence .', 'It freedom must protect nurture build .', 'If free , one respect us .', 'My second vision India ’ development .', 'For fifty years developing nation .', 'It time see developed nation .', 'We among top 5 nations world terms GDP .', 'We 10 percent growth rate areas .', 'Our poverty levels falling .', 'Our achievements globally recognized today .', 'Yet lack self-confidence see developed nation , self-reliant self-assured .', 'Isn ’ incorrect ?', 'I third vision .', 'India must stand world .', 'Because I believe unless India stands world , one respect us .', 'Only strength respects strength .', 'We must strong military power also economic power .', 'Both must go hand-in-hand .', 'My good fortune worked three great minds .', 'Dr. Vikram Sarabhai Dept .', 'space , Professor Satish Dhawan , succeeded Dr. Brahm Prakash , father nuclear material .', 'I lucky worked three closely consider great opportunity life .', 'I see four milestones career']\n"
     ]
    }
   ],
   "source": [
    "# Remove stopwords from paragraph using `lemmitization`\n",
    "paragraph_tokens = sent_tokenize(paragraph)\n",
    "\n",
    "# FIXME\n",
    "without_stop_word_sentences = []\n",
    "for sentence in paragraph_tokens:\n",
    "    lemma_sent = wordnet.lemmatize(sentence)\n",
    "    word_tokens = word_tokenize(lemma_sent)\n",
    "    words = [word for word in word_tokens if word not in stop_words]\n",
    "    sent = \" \".join(words)\n",
    "    without_stop_word_sentences.append(sent)\n",
    "\n",
    "print(without_stop_word_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4517797",
   "metadata": {
    "papermill": {
     "duration": 0.005218,
     "end_time": "2025-09-14T07:41:09.583243",
     "exception": false,
     "start_time": "2025-09-14T07:41:09.578025",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 11.380313,
   "end_time": "2025-09-14T07:41:10.407248",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-09-14T07:40:59.026935",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
